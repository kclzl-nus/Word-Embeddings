{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (13368, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A middle-school teacher in China has inked hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man convicted of killing the father and sist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avid rugby fan Prince Harry could barely watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Triple M Radio producer has been inundated w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Sally Forrest, an actress-dancer who graced th...\n",
       "1  A middle-school teacher in China has inked hun...\n",
       "2  A man convicted of killing the father and sist...\n",
       "3  Avid rugby fan Prince Harry could barely watch...\n",
       "4  A Triple M Radio producer has been inundated w..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/raw data/raw_data.csv', header=0, names=['text'], usecols=[1])\n",
    "print(f'Data Shape: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punctuations = string.punctuation\n",
    "def remove_punctuation(txt):\n",
    "    for char in punctuations:\n",
    "        if char in txt:\n",
    "            txt = txt.replace(char, \"\")\n",
    "    return txt\n",
    "\n",
    "# change to lower caps\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# remove punctuations\n",
    "data['text'] = data['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# read stopwords from data/raw data/stopwords.txt\n",
    "stop_words = []\n",
    "with open('./data/raw data/stopwords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip())\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    txt = [word for word in txt.split() if word not in stop_words]\n",
    "    return ' '.join(txt)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'among',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'around',\n",
       " 'as',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'at',\n",
       " 'away',\n",
       " 'b',\n",
       " 'back',\n",
       " 'backed',\n",
       " 'backing',\n",
       " 'backs',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'beings',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'come',\n",
       " 'could',\n",
       " 'd',\n",
       " 'did',\n",
       " 'differ',\n",
       " 'different',\n",
       " 'differently',\n",
       " 'do',\n",
       " 'does',\n",
       " 'done',\n",
       " 'down',\n",
       " 'down',\n",
       " 'downed',\n",
       " 'downing',\n",
       " 'downs',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'early',\n",
       " 'either',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'ends',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'evenly',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'f',\n",
       " 'face',\n",
       " 'faces',\n",
       " 'fact',\n",
       " 'facts',\n",
       " 'far',\n",
       " 'felt',\n",
       " 'few',\n",
       " 'find',\n",
       " 'finds',\n",
       " 'first',\n",
       " 'for',\n",
       " 'four',\n",
       " 'from',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'further',\n",
       " 'furthered',\n",
       " 'furthering',\n",
       " 'furthers',\n",
       " 'g',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'going',\n",
       " 'good',\n",
       " 'goods',\n",
       " 'got',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatest',\n",
       " 'group',\n",
       " 'grouped',\n",
       " 'grouping',\n",
       " 'groups',\n",
       " 'h',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'herself',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highest',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'i',\n",
       " 'if',\n",
       " 'important',\n",
       " 'in',\n",
       " 'interest',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interests',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kind',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'l',\n",
       " 'large',\n",
       " 'largely',\n",
       " 'last',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'least',\n",
       " 'less',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'longest',\n",
       " 'm',\n",
       " 'made',\n",
       " 'make',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'member',\n",
       " 'members',\n",
       " 'men',\n",
       " 'might',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needing',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'new',\n",
       " 'new',\n",
       " 'newer',\n",
       " 'newest',\n",
       " 'next',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'noone',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'old',\n",
       " 'older',\n",
       " 'oldest',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'opening',\n",
       " 'opens',\n",
       " 'or',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'ordering',\n",
       " 'orders',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'out',\n",
       " 'over',\n",
       " 'p',\n",
       " 'part',\n",
       " 'parted',\n",
       " 'parting',\n",
       " 'parts',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'place',\n",
       " 'places',\n",
       " 'point',\n",
       " 'pointed',\n",
       " 'pointing',\n",
       " 'points',\n",
       " 'possible',\n",
       " 'present',\n",
       " 'presented',\n",
       " 'presenting',\n",
       " 'presents',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'put',\n",
       " 'puts',\n",
       " 'q',\n",
       " 'quite',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'really',\n",
       " 'right',\n",
       " 'right',\n",
       " 'room',\n",
       " 'rooms',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'says',\n",
       " 'second',\n",
       " 'seconds',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'sees',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'showing',\n",
       " 'shows',\n",
       " 'side',\n",
       " 'sides',\n",
       " 'since',\n",
       " 'small',\n",
       " 'smaller',\n",
       " 'smallest',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'somewhere',\n",
       " 'state',\n",
       " 'states',\n",
       " 'still',\n",
       " 'still',\n",
       " 'such',\n",
       " 'sure',\n",
       " 't',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'therefore',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinks',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'thoughts',\n",
       " 'three',\n",
       " 'through',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'today',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'turning',\n",
       " 'turns',\n",
       " 'two',\n",
       " 'u',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'uses',\n",
       " 'v',\n",
       " 'very',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wanting',\n",
       " 'wants',\n",
       " 'was',\n",
       " 'way',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'well',\n",
       " 'wells',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whole',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'would',\n",
       " 'x',\n",
       " 'y',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'young',\n",
       " 'younger',\n",
       " 'youngest',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'z']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows of data: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [stoke, goalkeeper, jack, butland, signed, con...\n",
       "1    [ahead, weekends, premier, league, action, spo...\n",
       "2    [bizarre, tribunal, hospital, employee, awarde...\n",
       "3    [police, arrested, suspect, gangrape, elderly,...\n",
       "4    [jeremy, clarkson, return, top, gear, ‘his, ow...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each row into list of words\n",
    "data_lst = data['text'].apply(lambda txt: txt.split(\" \"))\n",
    "\n",
    "# select number of rows to be used as training data\n",
    "nrows = 200\n",
    "random_indices = np.random.randint(low=0, high=len(data_lst), size=nrows)\n",
    "data_lst = data_lst[random_indices].reset_index(drop=True)\n",
    "\n",
    "print(f'Number of rows of data: {len(data_lst)}')\n",
    "data_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 15062\n"
     ]
    }
   ],
   "source": [
    "# vocab dict\n",
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "for line in data_lst:\n",
    "    for word in line:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse_vocab dict\n",
    "inverse_vocab = {}\n",
    "for word, index in vocab.items():\n",
    "    inverse_vocab[index] = word\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "sequences = []\n",
    "for line in data_lst:\n",
    "    vectorized_line = [vocab[word] for word in line]\n",
    "    sequences.append(vectorized_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# choose 20 random sequences\n",
    "ntest = 20\n",
    "test_indices = np.random.randint(low=0, high=len(sequences), size=ntest)\n",
    "test_sequences = [sequences[i] for i in test_indices]\n",
    "train_sequences = [sequences[i] for i in range(len(sequences)) if i not in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate samples\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0,\n",
    "          shuffle=True)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.reshape(tf.constant([context_word], dtype=\"int64\"), (1,1))\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate testing data\n",
    "def generate_testing_data(sequences, vocab_size, window_size):\n",
    "    targets, contexts, labels = [], [], []\n",
    "    for sequence in tqdm(sequences):\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0)\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "        targets.append(target_word)\n",
    "        contexts.append(context_word)\n",
    "        labels.append(1)\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [02:30<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape: (320653,)\n",
      "contexts shape: (320653, 5)\n",
      "labels shape: (320653, 5)\n"
     ]
    }
   ],
   "source": [
    "# generate training data\n",
    "window_size = 5\n",
    "num_ns = 4\n",
    "vocab_size = len(vocab)\n",
    "seed = 4212\n",
    "\n",
    "targets, contexts, labels = generate_training_data(sequences=train_sequences,\n",
    "                                                 window_size=window_size,\n",
    "                                                 num_ns=num_ns,\n",
    "                                                 vocab_size=vocab_size,\n",
    "                                                 seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f'targets shape: {targets.shape}')\n",
    "print(f'contexts shape: {contexts.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 140.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets_test shape: (1200,)\n",
      "contexts_test shape: (1200,)\n",
      "labels_test shape: (1200,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate testing data\n",
    "targets_test, contexts_test, labels_test = generate_testing_data(sequences=test_sequences,\n",
    "                                                                    vocab_size=vocab_size,\n",
    "                                                                    window_size=window_size)\n",
    "\n",
    "targets_test = np.array(targets_test)\n",
    "contexts_test = np.array(contexts_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "print(f'targets_test shape: {targets_test.shape}')\n",
    "print(f'contexts_test shape: {contexts_test.shape}')\n",
    "print(f'labels_test shape: {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check on quality of training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 32\n",
      "target_word     : £4million\n",
      "context_indices : [   1   39 1601 1612 1079]\n",
      "context_words   : ['stoke', 'england', 'dwynwens', 'downgrade', 'womans']\n",
      "label           : [1 0 0 0 0]\n",
      "target  : 32\n",
      "context : [   1   39 1601 1612 1079]\n",
      "label   : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "print(f\"target_index    : {targets[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets[0]]}\")\n",
    "print(f\"context_indices : {contexts[0]}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c] for c in contexts[0]]}\")\n",
    "print(f\"label           : {labels[0]}\")\n",
    "\n",
    "print(\"target  :\", targets[0])\n",
    "print(\"context :\", contexts[0])\n",
    "print(\"label   :\", labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 392\n",
      "target_word     : police\n",
      "context_index : 3025\n",
      "context_word   : tuesday\n",
      "label           : 1\n",
      "target  : 392\n",
      "context : 3025\n",
      "label   : 1\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "print(f\"target_index    : {targets_test[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets_test[0]]}\")\n",
    "print(f\"context_index : {contexts_test[0]}\")\n",
    "print(f\"context_word   : {inverse_vocab[contexts_test[0]]}\")\n",
    "print(f\"label           : {labels_test[0]}\")\n",
    "\n",
    "print(\"target  :\", targets_test[0])\n",
    "print(\"context :\", contexts_test[0])\n",
    "print(\"label   :\", labels_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Objective Function for SGNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\min_{\\theta} = \\frac{1}{N} \\sum_{i=1}^{N} [log \\sigma(u_{ic}^T)  + \\sum_{k=1}^{K}log \\sigma(-u_{kc}^T v_{iw})]\n",
    "\n",
    "\\\\\n",
    "\\\\\n",
    "\\theta = [U, V]\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"Inputs a real number, outputs a real number\"\"\"\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "# define a local loss function\n",
    "# where it takes a params argument where params = [U, V]\n",
    "# U and V are the embedding matrices. Dimension of U : (n x |v|), Dimension of V : (|v| x n)\n",
    "# target is the index of the target word vector in the V matrix. Dimension: (1,)\n",
    "# context is the index of the context word vectors in the U matrix. Dimension: (n,)\n",
    "# returns a real number\n",
    "\n",
    "def local_loss(params,\n",
    "               target,\n",
    "               context):\n",
    "    \"\"\"\n",
    "    Input (example)\n",
    "    target = (188,)\n",
    "    context = (93, 40, 1648, 1659, 1109)\n",
    "    params = [V, U]\n",
    "        V: matrix of dim (n x |v|)\n",
    "        U: matrix of dim (|v| x n)\n",
    "            n = embedding dimension, |v| = vocab size\n",
    "\n",
    "    Outputs the local_loss -> real number\n",
    "    \"\"\"\n",
    "    target = target.astype(int)\n",
    "    context = context.astype(int)\n",
    "    V_embedding =params[0][0]\n",
    "    U_embedding = params[0][1]\n",
    "    \n",
    "    v_t = V_embedding.T[target]; print(f'v_t shape: {v_t.shape}') # shape (300,)\n",
    "    u_pos = U_embedding[context[0]]; print(f'u_pos shape: {u_pos.shape}')  # shape(300,)\n",
    "    u_neg = U_embedding[context[1:]]; print(f'u_neg shape: {u_neg.shape}')  # shape(4, 300)\n",
    "\n",
    "    return -jnp.log(sigmoid(jnp.dot(u_pos.T, v_t))) - jnp.sum(jnp.log(sigmoid(-jnp.dot(u_neg, v_t))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap the local loss across a batch of data points\n",
    "loss_all = jax.vmap(local_loss, in_axes=(None, 0, 0))\n",
    "\n",
    "@jax.jit\n",
    "def loss(params, targets, contexts):\n",
    "    \"\"\"return average of all the local losses\"\"\"\n",
    "    all_losses = loss_all(params, targets, contexts)\n",
    "    return jnp.mean(all_losses)\n",
    "\n",
    "# get the loss value and gradient\n",
    "loss_value_and_grad = jax.jit( jax.value_and_grad(loss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V shape: (200, 15062)\n",
      "U shape: (15062, 200)\n",
      "targets_data shape: (320653,)\n",
      "contexts_data shape: (320653, 5)\n",
      "labels_data shape: (320653, 5)\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "n = 200\n",
    "v = len(vocab)\n",
    "V = np.random.normal(0, 1, size=(n, v)) / np.sqrt(v)\n",
    "U = np.random.normal(0, 1, size=(v, n)) / np.sqrt(v)\n",
    "params = [(V, U)]\n",
    "targets_data = targets.astype(float)\n",
    "contexts_data = contexts.astype(float)\n",
    "labels_data = labels.astype(float)\n",
    "\n",
    "print(f'V shape: {V.shape}')\n",
    "print(f'U shape: {U.shape}')\n",
    "print(f'targets_data shape: {targets_data.shape}')\n",
    "print(f'contexts_data shape: {contexts_data.shape}')\n",
    "print(f'labels_data shape: {labels_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_t shape: (200,)\n",
      "u_pos shape: (200,)\n",
      "u_neg shape: (4, 200)\n",
      "Epoch 1/10 \t loss = 3.440905809402466 \t time = 20.04s\n",
      "Epoch 2/10 \t loss = 3.1278157234191895 \t time = 19.03s\n",
      "Epoch 3/10 \t loss = 2.8392622470855713 \t time = 18.95s\n",
      "Epoch 4/10 \t loss = 2.6382155418395996 \t time = 18.88s\n",
      "Epoch 5/10 \t loss = 2.492654323577881 \t time = 19.03s\n",
      "Epoch 6/10 \t loss = 2.382213592529297 \t time = 19.03s\n",
      "Epoch 7/10 \t loss = 2.295213222503662 \t time = 19.01s\n",
      "Epoch 8/10 \t loss = 2.2246487140655518 \t time = 19.01s\n",
      "Epoch 9/10 \t loss = 2.166088104248047 \t time = 19.04s\n",
      "Epoch 10/10 \t loss = 2.116589069366455 \t time = 19.08s\n"
     ]
    }
   ],
   "source": [
    "# train using stochastic gradient descent\n",
    "\n",
    "# number of training examples\n",
    "N = len(targets_data)\n",
    "\n",
    "# learning rate\n",
    "lr = 3.\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 10\n",
    "\n",
    "# batch size\n",
    "batch_size = 500\n",
    "\n",
    "# number of batches per epoch\n",
    "n_batches = N // batch_size\n",
    "\n",
    "# keep track of losses\n",
    "epoch_losses = []\n",
    "\n",
    "# training the network\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    # shuffle data\n",
    "    perm = np.random.permutation(N)\n",
    "    targets = targets[perm]\n",
    "    contexts = contexts[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    # half the learning rate every 25 epochs\n",
    "    # if epoch % 25 == 0 and epoch != 0:\n",
    "    #    lr /= 2.\n",
    "\n",
    "    # losses in each epoch\n",
    "    losses = []\n",
    "    for batch in range(n_batches):\n",
    "        targets_batch = targets_data[batch*batch_size: (batch+1)*batch_size]\n",
    "        contexts_batch = contexts_data[batch*batch_size: (batch+1)*batch_size]\n",
    "        labels_batch = labels_data[batch*batch_size: (batch+1)*batch_size]\n",
    "\n",
    "        # calculate and save losses\n",
    "        loss_value, gradient = loss_value_and_grad(params, targets_batch, contexts_batch)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        params = [(V - lr*dV, U - lr*dU) for (V, U), (dV, dU) in zip(params, gradient)]\n",
    "\n",
    "    epoch_losses.append(np.mean(losses))\n",
    "\n",
    "    end_time = time.time()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} \\t loss = {np.mean(epoch_losses)} \\t time = {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_ shape: (300, 14806)\n",
      "U_ shape: (14806, 300)\n"
     ]
    }
   ],
   "source": [
    "V_ = np.copy(params[0][0])\n",
    "U_ = np.copy(params[0][1])\n",
    "\n",
    "# check dimensions of U and V, 10 epoch, 1r = 3, 1.5, 0.75, batch size = 500\n",
    "print(f'V_ shape: {V_.shape}')\n",
    "print(f'U_ shape: {U_.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trained shape: (300, 14806)\n",
      "U_trained shape: (14806, 300)\n"
     ]
    }
   ],
   "source": [
    "# copy U and V\n",
    "V_trained = np.copy(params[0][0])\n",
    "U_trained = np.copy(params[0][1])\n",
    "\n",
    "# check dimensions of U and V, 100 epochs, lr = 1, batch_size = 500\n",
    "print(f'V_trained shape: {V_trained.shape}')\n",
    "print(f'U_trained shape: {U_trained.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate against test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "# define function that takes in an index and vocab size and returns the one-hot encoding\n",
    "def getOneHot(index, vocab_size):\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[index] = 1\n",
    "    return onehot\n",
    "\n",
    "# define softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# test getOneHot and softmax function\n",
    "print(getOneHot(1, 10))\n",
    "print(softmax(np.array([1, 2, 3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: (14806, 300)\n",
      "V shape: (300, 14806)\n"
     ]
    }
   ],
   "source": [
    "# check dimensions of U and V\n",
    "print(f'U shape: {U.shape}')\n",
    "print(f'V shape: {V.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'appears',\n",
       " 'to',\n",
       " 'be',\n",
       " 'homeless',\n",
       " 'woman',\n",
       " 'in',\n",
       " 'paris',\n",
       " 'reading',\n",
       " 'the',\n",
       " 'famed',\n",
       " 'magazine',\n",
       " 'is',\n",
       " 'full',\n",
       " 'surprisesand',\n",
       " 'voguemagazine',\n",
       " 'readers',\n",
       " 'even',\n",
       " 'unexpected',\n",
       " 'corners']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see first 20 words in the vocab\n",
    "test_words = list(vocab.keys())[20:40]\n",
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cosine similarity scores between 2 word vectors\n",
    "def similarity_score(target_word_embedding, context_word_embedding):\n",
    "    return np.dot(target_word_embedding, context_word_embedding) / (np.linalg.norm(target_word_embedding) * np.linalg.norm(context_word_embedding))\n",
    "\n",
    "# define a function that find the most similar words to a given word\n",
    "def most_similar_words(word, V, n=5):\n",
    "    scores = []\n",
    "    target_word_idx = vocab[word]\n",
    "    for i in range(V.shape[1]):\n",
    "        if i == target_word_idx or inverse_vocab[i] == '<pad>':\n",
    "            continue\n",
    "        scores.append((inverse_vocab[i], similarity_score(V[:, target_word_idx], V[:, i])))\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('subsequent', 0.869424), ('fortyseven', 0.86821645), ('hayler', 0.8639586), ('livein', 0.8605932), ('steal', 0.8605353)]\n",
      "[('lorries', 0.82925045), ('deported', 0.8283184), ('19090', 0.8239643), ('phased', 0.8218111), ('precious', 0.8180679)]\n"
     ]
    }
   ],
   "source": [
    "# check similarity between words\n",
    "# word: photos\n",
    "print(most_similar_words('photos', V_trained))\n",
    "\n",
    "# word: saturday\n",
    "print(most_similar_words('saturday', V_trained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hayler', 0.8091358), ('dangerous', 0.80723435), ('fortyseven', 0.8031516), ('franken', 0.80080813), ('steal', 0.7987832)]\n",
      "[('lorries', 0.77739894), ('deported', 0.7672179), ('19090', 0.7604412), ('precious', 0.7562994), ('unturned’', 0.7544449)]\n"
     ]
    }
   ],
   "source": [
    "# check similarity between words\n",
    "# word: photos\n",
    "print(most_similar_words('photos', V_))\n",
    "\n",
    "# word: saturday\n",
    "print(most_similar_words('saturday', V_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a forward pass through the skip-gram model\n",
    "\n",
    "# define the forward pass function\n",
    "def net(V, U, target_word_idx):\n",
    "    target_hot = getOneHot(target_word_idx, len(vocab))\n",
    "    return softmax( U @ V @ target_hot )\n",
    "\n",
    "def predict(word, V, U):\n",
    "    target_word_idx = vocab[word]\n",
    "    y_hat = net(V, U, target_word_idx)\n",
    "    # y_hat is the probability distribution over the vocab\n",
    "    # select the top 5 words with the highest probability\n",
    "    top_5 = np.argsort(y_hat)[-10:][::-1]\n",
    "    top_5_words = [inverse_vocab[i] for i in top_5]\n",
    "    return top_5_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', '', 'the', 'her', 'he', 'was', 'his', 'she', 'said', 'of']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'photos'\n",
    "y_hat = net(V_, U_, vocab[word])\n",
    "predict(word, V_, U_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
