{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (13368, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A middle-school teacher in China has inked hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man convicted of killing the father and sist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avid rugby fan Prince Harry could barely watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Triple M Radio producer has been inundated w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Sally Forrest, an actress-dancer who graced th...\n",
       "1  A middle-school teacher in China has inked hun...\n",
       "2  A man convicted of killing the father and sist...\n",
       "3  Avid rugby fan Prince Harry could barely watch...\n",
       "4  A Triple M Radio producer has been inundated w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/raw data/raw_data.csv', header=0, names=['text'], usecols=[1])\n",
    "print(f'Data Shape: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punctuations = string.punctuation\n",
    "def remove_punctuation(txt):\n",
    "    for char in punctuations:\n",
    "        if char in txt:\n",
    "            txt = txt.replace(char, \"\")\n",
    "    return txt\n",
    "\n",
    "# change to lower caps\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# remove punctuations\n",
    "data['text'] = data['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# read stopwords from data/raw data/stopwords.txt\n",
    "stop_words = []\n",
    "with open('./data/raw data/stopwords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip())\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    txt = [word for word in txt.split() if word not in stop_words]\n",
    "    return ' '.join(txt)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'among',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'around',\n",
       " 'as',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'at',\n",
       " 'away',\n",
       " 'b',\n",
       " 'back',\n",
       " 'backed',\n",
       " 'backing',\n",
       " 'backs',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'beings',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'come',\n",
       " 'could',\n",
       " 'd',\n",
       " 'did',\n",
       " 'differ',\n",
       " 'different',\n",
       " 'differently',\n",
       " 'do',\n",
       " 'does',\n",
       " 'done',\n",
       " 'down',\n",
       " 'down',\n",
       " 'downed',\n",
       " 'downing',\n",
       " 'downs',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'early',\n",
       " 'either',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'ends',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'evenly',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'f',\n",
       " 'face',\n",
       " 'faces',\n",
       " 'fact',\n",
       " 'facts',\n",
       " 'far',\n",
       " 'felt',\n",
       " 'few',\n",
       " 'find',\n",
       " 'finds',\n",
       " 'first',\n",
       " 'for',\n",
       " 'four',\n",
       " 'from',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'further',\n",
       " 'furthered',\n",
       " 'furthering',\n",
       " 'furthers',\n",
       " 'g',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'going',\n",
       " 'good',\n",
       " 'goods',\n",
       " 'got',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatest',\n",
       " 'group',\n",
       " 'grouped',\n",
       " 'grouping',\n",
       " 'groups',\n",
       " 'h',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'herself',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highest',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'i',\n",
       " 'if',\n",
       " 'important',\n",
       " 'in',\n",
       " 'interest',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interests',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kind',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'l',\n",
       " 'large',\n",
       " 'largely',\n",
       " 'last',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'least',\n",
       " 'less',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'longest',\n",
       " 'm',\n",
       " 'made',\n",
       " 'make',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'member',\n",
       " 'members',\n",
       " 'men',\n",
       " 'might',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needing',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'new',\n",
       " 'new',\n",
       " 'newer',\n",
       " 'newest',\n",
       " 'next',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'noone',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'old',\n",
       " 'older',\n",
       " 'oldest',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'opening',\n",
       " 'opens',\n",
       " 'or',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'ordering',\n",
       " 'orders',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'out',\n",
       " 'over',\n",
       " 'p',\n",
       " 'part',\n",
       " 'parted',\n",
       " 'parting',\n",
       " 'parts',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'place',\n",
       " 'places',\n",
       " 'point',\n",
       " 'pointed',\n",
       " 'pointing',\n",
       " 'points',\n",
       " 'possible',\n",
       " 'present',\n",
       " 'presented',\n",
       " 'presenting',\n",
       " 'presents',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'put',\n",
       " 'puts',\n",
       " 'q',\n",
       " 'quite',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'really',\n",
       " 'right',\n",
       " 'right',\n",
       " 'room',\n",
       " 'rooms',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'says',\n",
       " 'second',\n",
       " 'seconds',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'sees',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'showing',\n",
       " 'shows',\n",
       " 'side',\n",
       " 'sides',\n",
       " 'since',\n",
       " 'small',\n",
       " 'smaller',\n",
       " 'smallest',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'somewhere',\n",
       " 'state',\n",
       " 'states',\n",
       " 'still',\n",
       " 'still',\n",
       " 'such',\n",
       " 'sure',\n",
       " 't',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'therefore',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinks',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'thoughts',\n",
       " 'three',\n",
       " 'through',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'today',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'turning',\n",
       " 'turns',\n",
       " 'two',\n",
       " 'u',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'uses',\n",
       " 'v',\n",
       " 'very',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wanting',\n",
       " 'wants',\n",
       " 'was',\n",
       " 'way',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'well',\n",
       " 'wells',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whole',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'would',\n",
       " 'x',\n",
       " 'y',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'young',\n",
       " 'younger',\n",
       " 'youngest',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'z']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows of data: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [queen, celebrity, lifetime, role, kim, kardas...\n",
       "1    [brittany, maynards, widower, described, final...\n",
       "2    [airline, staff, trained, try, solve, customer...\n",
       "3    [football, fans, chelsea, supporters, train, s...\n",
       "4    [ufcs, undefeated, bantamweight, champion, row...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each row into list of words\n",
    "data_lst = data['text'].apply(lambda txt: txt.split(\" \"))\n",
    "\n",
    "# select number of rows to be used as training data\n",
    "nrows = 200\n",
    "random_indices = np.random.randint(low=0, high=len(data_lst), size=nrows)\n",
    "data_lst = data_lst[random_indices].reset_index(drop=True)\n",
    "\n",
    "print(f'Number of rows of data: {len(data_lst)}')\n",
    "data_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14611\n"
     ]
    }
   ],
   "source": [
    "# vocab dict\n",
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "for line in data_lst:\n",
    "    for word in line:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse_vocab dict\n",
    "inverse_vocab = {}\n",
    "for word, index in vocab.items():\n",
    "    inverse_vocab[index] = word\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "sequences = []\n",
    "for line in data_lst:\n",
    "    vectorized_line = [vocab[word] for word in line]\n",
    "    sequences.append(vectorized_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# choose 20 random sequences\n",
    "ntest = 20\n",
    "test_indices = np.random.randint(low=0, high=len(sequences), size=ntest)\n",
    "test_sequences = [sequences[i] for i in test_indices]\n",
    "train_sequences = [sequences[i] for i in range(len(sequences)) if i not in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate samples\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0,\n",
    "          shuffle=True)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.reshape(tf.constant([context_word], dtype=\"int64\"), (1,1))\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate testing data\n",
    "def generate_testing_data(sequences, vocab_size, window_size):\n",
    "    targets, contexts, labels = [], [], []\n",
    "    for sequence in tqdm(sequences):\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0)\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "        targets.append(target_word)\n",
    "        contexts.append(context_word)\n",
    "        labels.append(1)\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [02:22<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape: (307892,)\n",
      "contexts shape: (307892, 5)\n",
      "labels shape: (307892, 5)\n"
     ]
    }
   ],
   "source": [
    "# generate training data\n",
    "window_size = 5\n",
    "num_ns = 4\n",
    "vocab_size = len(vocab)\n",
    "seed = 4212\n",
    "\n",
    "targets, contexts, labels = generate_training_data(sequences=train_sequences,\n",
    "                                                 window_size=window_size,\n",
    "                                                 num_ns=num_ns,\n",
    "                                                 vocab_size=vocab_size,\n",
    "                                                 seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f'targets shape: {targets.shape}')\n",
    "print(f'contexts shape: {contexts.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 183.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets_test shape: (5570,)\n",
      "contexts_test shape: (5570,)\n",
      "labels_test shape: (5570,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate testing data\n",
    "targets_test, contexts_test, labels_test = generate_testing_data(sequences=test_sequences,\n",
    "                                                                    vocab_size=vocab_size,\n",
    "                                                                    window_size=window_size)\n",
    "\n",
    "targets_test = np.array(targets_test)\n",
    "contexts_test = np.array(contexts_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "print(f'targets_test shape: {targets_test.shape}')\n",
    "print(f'contexts_test shape: {contexts_test.shape}')\n",
    "print(f'labels_test shape: {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check on quality of training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 183\n",
      "target_word     : spoken\n",
      "context_indices : [ 140   39 1564 1575 1055]\n",
      "context_words   : ['moments', 'mexico', 'fee', 'raheems', 'sometimes']\n",
      "label           : [1 0 0 0 0]\n",
      "target  : 183\n",
      "context : [ 140   39 1564 1575 1055]\n",
      "label   : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "print(f\"target_index    : {targets[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets[0]]}\")\n",
    "print(f\"context_indices : {contexts[0]}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c] for c in contexts[0]]}\")\n",
    "print(f\"label           : {labels[0]}\")\n",
    "\n",
    "print(\"target  :\", targets[0])\n",
    "print(\"context :\", contexts[0])\n",
    "print(\"label   :\", labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 1826\n",
      "target_word     : able\n",
      "context_index : 12631\n",
      "context_word   : fiancé\n",
      "label           : 1\n",
      "target  : 1826\n",
      "context : 12631\n",
      "label   : 1\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "print(f\"target_index    : {targets_test[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets_test[0]]}\")\n",
    "print(f\"context_index : {contexts_test[0]}\")\n",
    "print(f\"context_word   : {inverse_vocab[contexts_test[0]]}\")\n",
    "print(f\"label           : {labels_test[0]}\")\n",
    "\n",
    "print(\"target  :\", targets_test[0])\n",
    "print(\"context :\", contexts_test[0])\n",
    "print(\"label   :\", labels_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Objective Function for SGNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\min_{\\theta} = \\frac{1}{N} \\sum_{i=1}^{N} [log \\sigma(u_{ic}^T)  + \\sum_{k=1}^{K}log \\sigma(-u_{kc}^T v_{iw})]\n",
    "\n",
    "\\\\\n",
    "\\\\\n",
    "\\theta = [U, V]\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"Inputs a real number, outputs a real number\"\"\"\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "# define a local loss function\n",
    "# where it takes a params argument where params = [U, V]\n",
    "# U and V are the embedding matrices. Dimension of U : (n x |v|), Dimension of V : (|v| x n)\n",
    "# target is the index of the target word vector in the V matrix. Dimension: (1,)\n",
    "# context is the index of the context word vectors in the U matrix. Dimension: (n,)\n",
    "# returns a real number\n",
    "\n",
    "def local_loss(params,\n",
    "               target,\n",
    "               context):\n",
    "    \"\"\"\n",
    "    Input (example)\n",
    "    target = (188,)\n",
    "    context = (93, 40, 1648, 1659, 1109)\n",
    "    params = [V, U]\n",
    "        V: matrix of dim (n x |v|)\n",
    "        U: matrix of dim (|v| x n)\n",
    "            n = embedding dimension, |v| = vocab size\n",
    "\n",
    "    Outputs the local_loss -> real number\n",
    "    \"\"\"\n",
    "    target = target.astype(int)\n",
    "    context = context.astype(int)\n",
    "    V_embedding =params[0][0]\n",
    "    U_embedding = params[0][1]\n",
    "    \n",
    "    v_t = V_embedding.T[target]; print(f'v_t shape: {v_t.shape}') # shape (300,)\n",
    "    u_pos = U_embedding[context[0]]; print(f'u_pos shape: {u_pos.shape}')  # shape(300,)\n",
    "    u_neg = U_embedding[context[1:]]; print(f'u_neg shape: {u_neg.shape}')  # shape(4, 300)\n",
    "\n",
    "    return -jnp.log(sigmoid(jnp.dot(u_pos.T, v_t))) - jnp.sum(jnp.log(sigmoid(-jnp.dot(u_neg, v_t))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap the local loss across a batch of data points\n",
    "loss_all = jax.vmap(local_loss, in_axes=(None, 0, 0))\n",
    "\n",
    "@jax.jit\n",
    "def loss(params, targets, contexts):\n",
    "    \"\"\"return average of all the local losses\"\"\"\n",
    "    all_losses = loss_all(params, targets, contexts)\n",
    "    return jnp.mean(all_losses)\n",
    "\n",
    "# get the loss value and gradient\n",
    "loss_value_and_grad = jax.jit( jax.value_and_grad(loss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V shape: (300, 14611)\n",
      "U shape: (14611, 300)\n",
      "targets_data shape: (307892,)\n",
      "contexts_data shape: (307892, 5)\n",
      "labels_data shape: (307892, 5)\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "n = 300\n",
    "v = len(vocab)\n",
    "V = np.random.normal(0, 1, size=(n, v)) / np.sqrt(v)\n",
    "U = np.random.normal(0, 1, size=(v, n)) / np.sqrt(v)\n",
    "params = [(V, U)]\n",
    "targets_data = targets.astype(float)\n",
    "contexts_data = contexts.astype(float)\n",
    "labels_data = labels.astype(float)\n",
    "\n",
    "print(f'V shape: {V.shape}')\n",
    "print(f'U shape: {U.shape}')\n",
    "print(f'targets_data shape: {targets_data.shape}')\n",
    "print(f'contexts_data shape: {contexts_data.shape}')\n",
    "print(f'labels_data shape: {labels_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_t shape: (300,)\n",
      "u_pos shape: (300,)\n",
      "u_neg shape: (4, 300)\n",
      "Epoch 1/100 \t loss = 3.369793176651001 \t time = 29.69s\n",
      "Epoch 2/100 \t loss = 3.0227952003479004 \t time = 31.93s\n",
      "Epoch 3/100 \t loss = 2.749447822570801 \t time = 28.70s\n",
      "Epoch 4/100 \t loss = 2.5635735988616943 \t time = 28.67s\n",
      "Epoch 5/100 \t loss = 2.430037260055542 \t time = 28.69s\n",
      "Epoch 6/100 \t loss = 2.328984022140503 \t time = 28.77s\n",
      "Epoch 7/100 \t loss = 2.249403238296509 \t time = 28.64s\n",
      "Epoch 8/100 \t loss = 2.1847991943359375 \t time = 28.68s\n",
      "Epoch 9/100 \t loss = 2.131100654602051 \t time = 28.54s\n",
      "Epoch 10/100 \t loss = 2.085622787475586 \t time = 28.63s\n",
      "Epoch 11/100 \t loss = 2.0465173721313477 \t time = 28.68s\n",
      "Epoch 12/100 \t loss = 2.0124671459198 \t time = 28.72s\n",
      "Epoch 13/100 \t loss = 1.9825040102005005 \t time = 32.35s\n",
      "Epoch 14/100 \t loss = 1.9558980464935303 \t time = 31.34s\n",
      "Epoch 15/100 \t loss = 1.9320868253707886 \t time = 29.01s\n",
      "Epoch 16/100 \t loss = 1.9106286764144897 \t time = 29.07s\n",
      "Epoch 17/100 \t loss = 1.8911702632904053 \t time = 29.21s\n",
      "Epoch 18/100 \t loss = 1.873424768447876 \t time = 28.82s\n",
      "Epoch 19/100 \t loss = 1.857155442237854 \t time = 28.45s\n",
      "Epoch 20/100 \t loss = 1.842165231704712 \t time = 28.72s\n",
      "Epoch 21/100 \t loss = 1.8282870054244995 \t time = 28.38s\n",
      "Epoch 22/100 \t loss = 1.8153786659240723 \t time = 28.37s\n",
      "Epoch 23/100 \t loss = 1.803317666053772 \t time = 28.54s\n",
      "Epoch 24/100 \t loss = 1.7919974327087402 \t time = 28.43s\n",
      "Epoch 25/100 \t loss = 1.7813255786895752 \t time = 28.59s\n",
      "Epoch 26/100 \t loss = 1.7712199687957764 \t time = 28.49s\n",
      "Epoch 27/100 \t loss = 1.7616080045700073 \t time = 28.54s\n",
      "Epoch 28/100 \t loss = 1.7524243593215942 \t time = 28.52s\n",
      "Epoch 29/100 \t loss = 1.7436102628707886 \t time = 28.21s\n",
      "Epoch 30/100 \t loss = 1.7351120710372925 \t time = 28.14s\n",
      "Epoch 31/100 \t loss = 1.7268813848495483 \t time = 28.26s\n",
      "Epoch 32/100 \t loss = 1.7188732624053955 \t time = 28.40s\n",
      "Epoch 33/100 \t loss = 1.711046814918518 \t time = 28.67s\n",
      "Epoch 34/100 \t loss = 1.7033648490905762 \t time = 28.47s\n",
      "Epoch 35/100 \t loss = 1.6957931518554688 \t time = 28.40s\n",
      "Epoch 36/100 \t loss = 1.6883004903793335 \t time = 28.41s\n",
      "Epoch 37/100 \t loss = 1.6808583736419678 \t time = 28.41s\n",
      "Epoch 38/100 \t loss = 1.6734411716461182 \t time = 28.30s\n",
      "Epoch 39/100 \t loss = 1.6660254001617432 \t time = 28.01s\n",
      "Epoch 40/100 \t loss = 1.65859055519104 \t time = 26.30s\n",
      "Epoch 41/100 \t loss = 1.6511178016662598 \t time = 25.41s\n",
      "Epoch 42/100 \t loss = 1.643591046333313 \t time = 25.56s\n",
      "Epoch 43/100 \t loss = 1.6359961032867432 \t time = 25.22s\n",
      "Epoch 44/100 \t loss = 1.6283206939697266 \t time = 25.56s\n",
      "Epoch 45/100 \t loss = 1.6205545663833618 \t time = 25.51s\n",
      "Epoch 46/100 \t loss = 1.61268949508667 \t time = 25.56s\n",
      "Epoch 47/100 \t loss = 1.6047184467315674 \t time = 25.58s\n",
      "Epoch 48/100 \t loss = 1.596636176109314 \t time = 25.43s\n",
      "Epoch 49/100 \t loss = 1.5884389877319336 \t time = 28.52s\n",
      "Epoch 50/100 \t loss = 1.5801246166229248 \t time = 28.51s\n",
      "Epoch 51/100 \t loss = 1.5716992616653442 \t time = 27.99s\n",
      "Epoch 52/100 \t loss = 1.5633759498596191 \t time = 27.57s\n",
      "Epoch 53/100 \t loss = 1.5551478862762451 \t time = 28.65s\n",
      "Epoch 54/100 \t loss = 1.547008752822876 \t time = 27.59s\n",
      "Epoch 55/100 \t loss = 1.5389529466629028 \t time = 27.56s\n",
      "Epoch 56/100 \t loss = 1.5309749841690063 \t time = 28.23s\n",
      "Epoch 57/100 \t loss = 1.5230697393417358 \t time = 28.77s\n",
      "Epoch 58/100 \t loss = 1.515232801437378 \t time = 27.60s\n",
      "Epoch 59/100 \t loss = 1.507460117340088 \t time = 27.53s\n",
      "Epoch 60/100 \t loss = 1.4997482299804688 \t time = 28.27s\n",
      "Epoch 61/100 \t loss = 1.492093563079834 \t time = 27.62s\n",
      "Epoch 62/100 \t loss = 1.4844932556152344 \t time = 27.57s\n",
      "Epoch 63/100 \t loss = 1.4769445657730103 \t time = 27.15s\n",
      "Epoch 64/100 \t loss = 1.4694453477859497 \t time = 27.53s\n",
      "Epoch 65/100 \t loss = 1.4619929790496826 \t time = 27.53s\n",
      "Epoch 66/100 \t loss = 1.4545856714248657 \t time = 27.57s\n",
      "Epoch 67/100 \t loss = 1.4472217559814453 \t time = 27.50s\n",
      "Epoch 68/100 \t loss = 1.4398996829986572 \t time = 27.40s\n",
      "Epoch 69/100 \t loss = 1.4326179027557373 \t time = 27.56s\n",
      "Epoch 70/100 \t loss = 1.4253754615783691 \t time = 27.57s\n",
      "Epoch 71/100 \t loss = 1.4181711673736572 \t time = 27.61s\n",
      "Epoch 72/100 \t loss = 1.4110041856765747 \t time = 27.62s\n",
      "Epoch 73/100 \t loss = 1.4038736820220947 \t time = 27.71s\n",
      "Epoch 74/100 \t loss = 1.39677894115448 \t time = 28.00s\n",
      "Epoch 75/100 \t loss = 1.3897194862365723 \t time = 27.78s\n",
      "Epoch 76/100 \t loss = 1.3826960325241089 \t time = 27.73s\n",
      "Epoch 77/100 \t loss = 1.375780701637268 \t time = 27.74s\n",
      "Epoch 78/100 \t loss = 1.3689695596694946 \t time = 27.76s\n",
      "Epoch 79/100 \t loss = 1.3622589111328125 \t time = 27.69s\n",
      "Epoch 80/100 \t loss = 1.3556452989578247 \t time = 27.62s\n",
      "Epoch 81/100 \t loss = 1.3491252660751343 \t time = 27.66s\n",
      "Epoch 82/100 \t loss = 1.342695713043213 \t time = 28.69s\n",
      "Epoch 83/100 \t loss = 1.3363536596298218 \t time = 27.84s\n",
      "Epoch 84/100 \t loss = 1.3300962448120117 \t time = 27.74s\n",
      "Epoch 85/100 \t loss = 1.323920726776123 \t time = 27.76s\n",
      "Epoch 86/100 \t loss = 1.3178244829177856 \t time = 28.52s\n",
      "Epoch 87/100 \t loss = 1.3118051290512085 \t time = 27.78s\n",
      "Epoch 88/100 \t loss = 1.3058602809906006 \t time = 27.75s\n",
      "Epoch 89/100 \t loss = 1.2999876737594604 \t time = 27.77s\n",
      "Epoch 90/100 \t loss = 1.2941850423812866 \t time = 27.70s\n",
      "Epoch 91/100 \t loss = 1.2884505987167358 \t time = 28.77s\n",
      "Epoch 92/100 \t loss = 1.2827821969985962 \t time = 28.34s\n",
      "Epoch 93/100 \t loss = 1.2771780490875244 \t time = 27.73s\n",
      "Epoch 94/100 \t loss = 1.2716363668441772 \t time = 27.77s\n",
      "Epoch 95/100 \t loss = 1.266155481338501 \t time = 27.85s\n",
      "Epoch 96/100 \t loss = 1.260733723640442 \t time = 27.84s\n",
      "Epoch 97/100 \t loss = 1.2553695440292358 \t time = 27.72s\n",
      "Epoch 98/100 \t loss = 1.2500616312026978 \t time = 27.76s\n",
      "Epoch 99/100 \t loss = 1.2448081970214844 \t time = 27.78s\n",
      "Epoch 100/100 \t loss = 1.2396082878112793 \t time = 27.74s\n"
     ]
    }
   ],
   "source": [
    "# train using stochastic gradient descent\n",
    "\n",
    "# number of training examples\n",
    "N = len(targets_data)\n",
    "\n",
    "# learning rate\n",
    "lr = 3.\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "# batch size\n",
    "batch_size = 500\n",
    "\n",
    "# number of batches per epoch\n",
    "n_batches = N // batch_size\n",
    "\n",
    "# keep track of losses\n",
    "epoch_losses = []\n",
    "\n",
    "# training the network\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    # shuffle data\n",
    "    perm = np.random.permutation(N)\n",
    "    targets = targets[perm]\n",
    "    contexts = contexts[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    # half the learning rate every 25 epochs\n",
    "    if epoch == 50 or epoch == 75:\n",
    "        lr /= 2.\n",
    "\n",
    "    # losses in each epoch\n",
    "    losses = []\n",
    "    for batch in range(n_batches):\n",
    "        targets_batch = targets_data[batch*batch_size: (batch+1)*batch_size]\n",
    "        contexts_batch = contexts_data[batch*batch_size: (batch+1)*batch_size]\n",
    "        labels_batch = labels_data[batch*batch_size: (batch+1)*batch_size]\n",
    "\n",
    "        # calculate and save losses\n",
    "        loss_value, gradient = loss_value_and_grad(params, targets_batch, contexts_batch)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        params = [(V - lr*dV, U - lr*dU) for (V, U), (dV, dU) in zip(params, gradient)]\n",
    "\n",
    "    epoch_losses.append(np.mean(losses))\n",
    "\n",
    "    end_time = time.time()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} \\t loss = {np.mean(epoch_losses)} \\t time = {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V_ = np.copy(params[0][0])\n",
    "# U_ = np.copy(params[0][1])\n",
    "\n",
    "# # check dimensions of U and V, 10 epoch, 1r = 3, 1.5, 0.75, batch size = 500\n",
    "# print(f'V_ shape: {V_.shape}')\n",
    "# print(f'U_ shape: {U_.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trained shape: (300, 14611)\n",
      "U_trained shape: (14611, 300)\n"
     ]
    }
   ],
   "source": [
    "# copy U and V\n",
    "V_trained = np.copy(params[0][0])\n",
    "U_trained = np.copy(params[0][1])\n",
    "\n",
    "# check dimensions of U and V, 100 epochs, lr = 1, batch_size = 500\n",
    "print(f'V_trained shape: {V_trained.shape}')\n",
    "print(f'U_trained shape: {U_trained.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate against test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "# define function that takes in an index and vocab size and returns the one-hot encoding\n",
    "def getOneHot(index, vocab_size):\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[index] = 1\n",
    "    return onehot\n",
    "\n",
    "# define softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# test getOneHot and softmax function\n",
    "print(getOneHot(1, 10))\n",
    "print(softmax(np.array([1, 2, 3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: (14611, 300)\n",
      "V shape: (300, 14611)\n"
     ]
    }
   ],
   "source": [
    "# check dimensions of U and V\n",
    "print(f'U shape: {U.shape}')\n",
    "print(f'V shape: {V.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gathering',\n",
       " 'huge',\n",
       " 'support',\n",
       " 'base',\n",
       " 'social',\n",
       " 'media',\n",
       " 'jimena',\n",
       " 'sanchez',\n",
       " '30',\n",
       " 'yearold',\n",
       " 'sports',\n",
       " 'latin',\n",
       " 'american',\n",
       " 'division',\n",
       " 'fox',\n",
       " 'deportes',\n",
       " 'scroll',\n",
       " 'video',\n",
       " 'left',\n",
       " 'mexico']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see first 20 words in the vocab\n",
    "test_words = list(vocab.keys())[20:40]\n",
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cosine similarity scores between 2 word vectors\n",
    "def similarity_score(target_word_embedding, context_word_embedding):\n",
    "    return np.dot(target_word_embedding, context_word_embedding) / (np.linalg.norm(target_word_embedding) * np.linalg.norm(context_word_embedding))\n",
    "\n",
    "# define a function that find the most similar words to a given word\n",
    "def most_similar_words(word, V, n=5):\n",
    "    scores = []\n",
    "    target_word_idx = vocab[word]\n",
    "    for i in range(V.shape[1]):\n",
    "        if i == target_word_idx or inverse_vocab[i] == '<pad>':\n",
    "            continue\n",
    "        scores.append((inverse_vocab[i], similarity_score(V[:, target_word_idx], V[:, i])))\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check similarity between words\n",
    "# word: photos\n",
    "# print(most_similar_words('photos', V_trained))\n",
    "\n",
    "# word: saturday\n",
    "# print(most_similar_words('saturday', V_trained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a forward pass through the skip-gram model\n",
    "\n",
    "# define the forward pass function\n",
    "def net(V, U, target_word_idx):\n",
    "    target_hot = getOneHot(target_word_idx, len(vocab))\n",
    "    return softmax( U @ V @ target_hot )\n",
    "\n",
    "def predict(word, V, U):\n",
    "    target_word_idx = vocab[word]\n",
    "    y_hat = net(V, U, target_word_idx)\n",
    "    # y_hat is the probability distribution over the vocab\n",
    "    # select the top 5 words with the highest probability\n",
    "    top_5 = np.argsort(y_hat)[-10:][::-1]\n",
    "    top_5_words = [inverse_vocab[i] for i in top_5]\n",
    "    return top_5_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: fog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['former',\n",
       " 'helicopter',\n",
       " 'night',\n",
       " 'clarkson',\n",
       " 'mars',\n",
       " 'russian',\n",
       " 'black',\n",
       " 'murray',\n",
       " 'scurfield',\n",
       " 'crashed']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select 1 word from vocab\n",
    "word = np.random.choice(list(vocab.keys()))\n",
    "y_hat = net(V_trained, U_trained, vocab[word])\n",
    "print(f'Word: {word}')\n",
    "predict(word, V_trained, U_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
