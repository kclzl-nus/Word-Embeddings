{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (13368, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A middle-school teacher in China has inked hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man convicted of killing the father and sist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avid rugby fan Prince Harry could barely watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Triple M Radio producer has been inundated w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Sally Forrest, an actress-dancer who graced th...\n",
       "1  A middle-school teacher in China has inked hun...\n",
       "2  A man convicted of killing the father and sist...\n",
       "3  Avid rugby fan Prince Harry could barely watch...\n",
       "4  A Triple M Radio producer has been inundated w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/raw data/raw_data.csv', header=0, names=['text'], usecols=[1])\n",
    "print(f'Data Shape: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punctuations = string.punctuation\n",
    "def remove_punctuation(txt):\n",
    "    for char in punctuations:\n",
    "        if char in txt:\n",
    "            txt = txt.replace(char, \"\")\n",
    "    return txt\n",
    "\n",
    "# change to lower caps\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# remove punctuations\n",
    "data['text'] = data['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# read stopwords from data/raw data/stopwords.txt\n",
    "stop_words = []\n",
    "with open('./data/raw data/stopwords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip())\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    txt = [word for word in txt.split() if word not in stop_words]\n",
    "    return ' '.join(txt)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows of data: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [cornish, coast, toilet, block, stunning, sea,...\n",
       "1    [thousands, tourists, residents, basking, hot,...\n",
       "2    [rare, series, interlinked, operations, 12, pa...\n",
       "3    [jonjo, oâ€™neill, doubts, shouting, particular,...\n",
       "4    [motherofthree, credited, slice, pizza, saving...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each row into list of words\n",
    "data_lst = data['text'].apply(lambda txt: txt.split(\" \"))\n",
    "\n",
    "# select number of rows to be used as training data\n",
    "nrows = 200\n",
    "random_indices = np.random.randint(low=0, high=len(data_lst), size=nrows)\n",
    "data_lst = data_lst[random_indices].reset_index(drop=True)\n",
    "\n",
    "print(f'Number of rows of data: {len(data_lst)}')\n",
    "data_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14829\n"
     ]
    }
   ],
   "source": [
    "# vocab dict\n",
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "for line in data_lst:\n",
    "    for word in line:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse_vocab dict\n",
    "inverse_vocab = {}\n",
    "for word, index in vocab.items():\n",
    "    inverse_vocab[index] = word\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "sequences = []\n",
    "for line in data_lst:\n",
    "    vectorized_line = [vocab[word] for word in line]\n",
    "    sequences.append(vectorized_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# choose 20 random sequences\n",
    "ntest = 20\n",
    "test_indices = np.random.randint(low=0, high=len(sequences), size=ntest)\n",
    "test_sequences = [sequences[i] for i in test_indices]\n",
    "train_sequences = [sequences[i] for i in range(len(sequences)) if i not in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate samples\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0,\n",
    "          shuffle=True)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.reshape(tf.constant([context_word], dtype=\"int64\"), (1,1))\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate testing data\n",
    "def generate_testing_data(sequences, vocab_size, window_size):\n",
    "    targets, contexts, labels = [], [], []\n",
    "    for sequence in tqdm(sequences):\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0)\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "        targets.append(target_word)\n",
    "        contexts.append(context_word)\n",
    "        labels.append(1)\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [02:30<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape: (316229,)\n",
      "contexts shape: (316229, 5)\n",
      "labels shape: (316229, 5)\n"
     ]
    }
   ],
   "source": [
    "# generate training data\n",
    "window_size = 5\n",
    "num_ns = 4\n",
    "vocab_size = len(vocab)\n",
    "seed = 4212\n",
    "\n",
    "targets, contexts, labels = generate_training_data(sequences=train_sequences,\n",
    "                                                 window_size=window_size,\n",
    "                                                 num_ns=num_ns,\n",
    "                                                 vocab_size=vocab_size,\n",
    "                                                 seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f'targets shape: {targets.shape}')\n",
    "print(f'contexts shape: {contexts.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 165.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets_test shape: (2280,)\n",
      "contexts_test shape: (2280,)\n",
      "labels_test shape: (2280,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate testing data\n",
    "targets_test, contexts_test, labels_test = generate_testing_data(sequences=test_sequences,\n",
    "                                                                    vocab_size=vocab_size,\n",
    "                                                                    window_size=window_size)\n",
    "\n",
    "targets_test = np.array(targets_test)\n",
    "contexts_test = np.array(contexts_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "print(f'targets_test shape: {targets_test.shape}')\n",
    "print(f'contexts_test shape: {contexts_test.shape}')\n",
    "print(f'labels_test shape: {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check on quality of training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 128\n",
      "target_word     : frank\n",
      "context_indices : [ 132   39 1582 1593 1067]\n",
      "context_words   : ['told', 'sale', 'monitored', 'authority', 'quiet']\n",
      "label           : [1 0 0 0 0]\n",
      "target  : 128\n",
      "context : [ 132   39 1582 1593 1067]\n",
      "label   : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "print(f\"target_index    : {targets[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets[0]]}\")\n",
    "print(f\"context_indices : {contexts[0]}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c] for c in contexts[0]]}\")\n",
    "print(f\"label           : {labels[0]}\")\n",
    "\n",
    "print(\"target  :\", targets[0])\n",
    "print(\"context :\", contexts[0])\n",
    "print(\"label   :\", labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 11131\n",
      "target_word     : rome\n",
      "context_index : 6769\n",
      "context_word   : starts\n",
      "label           : 1\n",
      "target  : 11131\n",
      "context : 6769\n",
      "label   : 1\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "print(f\"target_index    : {targets_test[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets_test[0]]}\")\n",
    "print(f\"context_index : {contexts_test[0]}\")\n",
    "print(f\"context_word   : {inverse_vocab[contexts_test[0]]}\")\n",
    "print(f\"label           : {labels_test[0]}\")\n",
    "\n",
    "print(\"target  :\", targets_test[0])\n",
    "print(\"context :\", contexts_test[0])\n",
    "print(\"label   :\", labels_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Objective Function for SGNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\min_{\\theta} = \\frac{1}{N} \\sum_{i=1}^{N} [log \\sigma(u_{ic}^T)  + \\sum_{k=1}^{K}log \\sigma(-u_{kc}^T v_{iw})]\n",
    "\n",
    "\\\\\n",
    "\\\\\n",
    "\\theta = [U, V]\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"Inputs a real number, outputs a real number\"\"\"\n",
    "    return 1 / (1 + jnp.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a local loss function\n",
    "# where it takes a params argument where params = [U, V]\n",
    "# U and V are the embedding matrices. Dimension of U : (n x |v|), Dimension of V : (|v| x n)\n",
    "# target is the index of the target word vector in the V matrix. Dimension: (1,)\n",
    "# context is the index of the context word vectors in the U matrix. Dimension: (n,)\n",
    "# returns a real number\n",
    "\n",
    "def local_loss(params,\n",
    "               target,\n",
    "               context):\n",
    "    \"\"\"\n",
    "    Input (example)\n",
    "    target = (188,)\n",
    "    context = (93, 40, 1648, 1659, 1109)\n",
    "    params = [V, U]\n",
    "        V: matrix of dim (n x |v|)\n",
    "        U: matrix of dim (|v| x n)\n",
    "            n = embedding dimension, |v| = vocab size\n",
    "\n",
    "    Outputs the local_loss -> real number\n",
    "    \"\"\"\n",
    "    target = target.astype(int)\n",
    "    context = context.astype(int)\n",
    "    V_embedding =params[0][0]\n",
    "    U_embedding = params[0][1]\n",
    "    \n",
    "    v_t = V_embedding.T[target]; print(f'v_t shape: {v_t.shape}') # shape (300,)\n",
    "    u_pos = U_embedding[context[0]]; print(f'u_pos shape: {u_pos.shape}')  # shape(300,)\n",
    "    u_neg = U_embedding[context[1:]]; print(f'u_neg shape: {u_neg.shape}')  # shape(4, 300)\n",
    "\n",
    "    return -jnp.log(sigmoid(jnp.dot(u_pos.T, v_t))) - jnp.sum(jnp.log(sigmoid(-jnp.dot(u_neg, v_t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap the local loss across a batch of data points\n",
    "loss_all = jax.vmap(local_loss, in_axes=(None, 0, 0))\n",
    "\n",
    "@jax.jit\n",
    "def loss(params, targets, contexts):\n",
    "    \"\"\"return average of all the local losses\"\"\"\n",
    "    all_losses = loss_all(params, targets, contexts)\n",
    "    return jnp.mean(all_losses)\n",
    "\n",
    "# get the loss value and gradient\n",
    "loss_value_and_grad = jax.jit( jax.value_and_grad(loss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V shape: (300, 14829)\n",
      "U shape: (14829, 300)\n",
      "targets_data shape: (316229,)\n",
      "contexts_data shape: (316229, 5)\n",
      "labels_data shape: (316229, 5)\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "n = 300\n",
    "v = len(vocab)\n",
    "V = np.random.normal(0, 1, size=(n, v)) / np.sqrt(v)\n",
    "U = np.random.normal(0, 1, size=(v, n)) / np.sqrt(v)\n",
    "params = [(V, U)]\n",
    "targets_data = targets.astype(float)\n",
    "contexts_data = contexts.astype(float)\n",
    "labels_data = labels.astype(float)\n",
    "\n",
    "print(f'V shape: {V.shape}')\n",
    "print(f'U shape: {U.shape}')\n",
    "print(f'targets_data shape: {targets_data.shape}')\n",
    "print(f'contexts_data shape: {contexts_data.shape}')\n",
    "print(f'labels_data shape: {labels_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_t shape: (300,)\n",
      "u_pos shape: (300,)\n",
      "u_neg shape: (4, 300)\n",
      "Epoch 1/100 \t loss = 3.2259926795959473 \t time = 18.18s\n",
      "Epoch 2/100 \t loss = 2.746544361114502 \t time = 16.72s\n",
      "Epoch 3/100 \t loss = 2.474778890609741 \t time = 16.10s\n",
      "Epoch 4/100 \t loss = 2.303170680999756 \t time = 16.31s\n",
      "Epoch 5/100 \t loss = 2.183927536010742 \t time = 15.84s\n",
      "Epoch 6/100 \t loss = 2.0956599712371826 \t time = 16.00s\n",
      "Epoch 7/100 \t loss = 2.0273597240448 \t time = 17.33s\n",
      "Epoch 8/100 \t loss = 1.9727609157562256 \t time = 16.10s\n",
      "Epoch 9/100 \t loss = 1.9279817342758179 \t time = 16.50s\n",
      "Epoch 10/100 \t loss = 1.8905121088027954 \t time = 15.98s\n",
      "Epoch 11/100 \t loss = 1.8586186170578003 \t time = 16.65s\n",
      "Epoch 12/100 \t loss = 1.8310546875 \t time = 17.51s\n",
      "Epoch 13/100 \t loss = 1.8068960905075073 \t time = 17.60s\n",
      "Epoch 14/100 \t loss = 1.7854450941085815 \t time = 16.63s\n",
      "Epoch 15/100 \t loss = 1.766162395477295 \t time = 16.03s\n",
      "Epoch 16/100 \t loss = 1.7485976219177246 \t time = 16.74s\n",
      "Epoch 17/100 \t loss = 1.7324066162109375 \t time = 15.81s\n",
      "Epoch 18/100 \t loss = 1.7173000574111938 \t time = 15.86s\n",
      "Epoch 19/100 \t loss = 1.7030539512634277 \t time = 15.85s\n",
      "Epoch 20/100 \t loss = 1.689465880393982 \t time = 15.86s\n",
      "Epoch 21/100 \t loss = 1.6763614416122437 \t time = 15.84s\n",
      "Epoch 22/100 \t loss = 1.664042592048645 \t time = 15.85s\n",
      "Epoch 23/100 \t loss = 1.652381181716919 \t time = 16.04s\n",
      "Epoch 24/100 \t loss = 1.6412843465805054 \t time = 15.93s\n",
      "Epoch 25/100 \t loss = 1.6306670904159546 \t time = 16.96s\n",
      "Epoch 26/100 \t loss = 1.6204533576965332 \t time = 16.53s\n",
      "Epoch 27/100 \t loss = 1.610579013824463 \t time = 15.43s\n",
      "Epoch 28/100 \t loss = 1.6009914875030518 \t time = 15.36s\n",
      "Epoch 29/100 \t loss = 1.5916463136672974 \t time = 15.30s\n",
      "Epoch 30/100 \t loss = 1.582499623298645 \t time = 15.26s\n",
      "Epoch 31/100 \t loss = 1.573518991470337 \t time = 15.23s\n",
      "Epoch 32/100 \t loss = 1.564673662185669 \t time = 15.34s\n",
      "Epoch 33/100 \t loss = 1.5559327602386475 \t time = 15.31s\n",
      "Epoch 34/100 \t loss = 1.547275424003601 \t time = 15.29s\n",
      "Epoch 35/100 \t loss = 1.5386830568313599 \t time = 15.27s\n",
      "Epoch 36/100 \t loss = 1.5301371812820435 \t time = 15.29s\n",
      "Epoch 37/100 \t loss = 1.5216234922409058 \t time = 17.87s\n",
      "Epoch 38/100 \t loss = 1.5131254196166992 \t time = 17.39s\n",
      "Epoch 39/100 \t loss = 1.5046350955963135 \t time = 14.95s\n",
      "Epoch 40/100 \t loss = 1.4961427450180054 \t time = 14.76s\n",
      "Epoch 41/100 \t loss = 1.4876389503479004 \t time = 14.79s\n",
      "Epoch 42/100 \t loss = 1.4791196584701538 \t time = 14.80s\n",
      "Epoch 43/100 \t loss = 1.4705805778503418 \t time = 14.78s\n",
      "Epoch 44/100 \t loss = 1.462015986442566 \t time = 14.80s\n",
      "Epoch 45/100 \t loss = 1.4534225463867188 \t time = 16.07s\n",
      "Epoch 46/100 \t loss = 1.4447990655899048 \t time = 18.28s\n",
      "Epoch 47/100 \t loss = 1.436144232749939 \t time = 15.88s\n",
      "Epoch 48/100 \t loss = 1.4274555444717407 \t time = 15.83s\n",
      "Epoch 49/100 \t loss = 1.4187349081039429 \t time = 15.85s\n",
      "Epoch 50/100 \t loss = 1.4099845886230469 \t time = 15.77s\n",
      "Epoch 51/100 \t loss = 1.4012218713760376 \t time = 15.85s\n",
      "Epoch 52/100 \t loss = 1.3926142454147339 \t time = 15.83s\n",
      "Epoch 53/100 \t loss = 1.3841522932052612 \t time = 15.82s\n",
      "Epoch 54/100 \t loss = 1.375826120376587 \t time = 15.83s\n",
      "Epoch 55/100 \t loss = 1.3676315546035767 \t time = 15.85s\n",
      "Epoch 56/100 \t loss = 1.3595607280731201 \t time = 15.85s\n",
      "Epoch 57/100 \t loss = 1.3516064882278442 \t time = 15.80s\n",
      "Epoch 58/100 \t loss = 1.3437639474868774 \t time = 17.15s\n",
      "Epoch 59/100 \t loss = 1.336028814315796 \t time = 16.96s\n",
      "Epoch 60/100 \t loss = 1.3283953666687012 \t time = 15.85s\n",
      "Epoch 61/100 \t loss = 1.3208590745925903 \t time = 15.85s\n",
      "Epoch 62/100 \t loss = 1.3134158849716187 \t time = 16.28s\n",
      "Epoch 63/100 \t loss = 1.3060616254806519 \t time = 16.60s\n",
      "Epoch 64/100 \t loss = 1.2987940311431885 \t time = 15.85s\n",
      "Epoch 65/100 \t loss = 1.2916072607040405 \t time = 16.46s\n",
      "Epoch 66/100 \t loss = 1.2844997644424438 \t time = 15.89s\n",
      "Epoch 67/100 \t loss = 1.277469515800476 \t time = 15.80s\n",
      "Epoch 68/100 \t loss = 1.270512580871582 \t time = 15.77s\n",
      "Epoch 69/100 \t loss = 1.2636276483535767 \t time = 15.78s\n",
      "Epoch 70/100 \t loss = 1.2568119764328003 \t time = 15.81s\n",
      "Epoch 71/100 \t loss = 1.2500629425048828 \t time = 15.83s\n",
      "Epoch 72/100 \t loss = 1.2433785200119019 \t time = 15.78s\n",
      "Epoch 73/100 \t loss = 1.2367573976516724 \t time = 16.29s\n",
      "Epoch 74/100 \t loss = 1.2301982641220093 \t time = 15.03s\n",
      "Epoch 75/100 \t loss = 1.2236990928649902 \t time = 14.76s\n",
      "Epoch 76/100 \t loss = 1.2172616720199585 \t time = 14.61s\n",
      "Epoch 77/100 \t loss = 1.2109358310699463 \t time = 14.64s\n",
      "Epoch 78/100 \t loss = 1.2047182321548462 \t time = 14.58s\n",
      "Epoch 79/100 \t loss = 1.1986039876937866 \t time = 14.64s\n",
      "Epoch 80/100 \t loss = 1.1925899982452393 \t time = 14.61s\n",
      "Epoch 81/100 \t loss = 1.1866732835769653 \t time = 14.95s\n",
      "Epoch 82/100 \t loss = 1.1808489561080933 \t time = 18.53s\n",
      "Epoch 83/100 \t loss = 1.1751152276992798 \t time = 14.95s\n",
      "Epoch 84/100 \t loss = 1.1694693565368652 \t time = 17.45s\n",
      "Epoch 85/100 \t loss = 1.1639076471328735 \t time = 14.79s\n",
      "Epoch 86/100 \t loss = 1.1584281921386719 \t time = 14.75s\n",
      "Epoch 87/100 \t loss = 1.1530269384384155 \t time = 14.74s\n",
      "Epoch 88/100 \t loss = 1.147702932357788 \t time = 14.74s\n",
      "Epoch 89/100 \t loss = 1.1424530744552612 \t time = 14.80s\n",
      "Epoch 90/100 \t loss = 1.1372750997543335 \t time = 14.80s\n",
      "Epoch 91/100 \t loss = 1.1321680545806885 \t time = 14.80s\n",
      "Epoch 92/100 \t loss = 1.127150535583496 \t time = 14.76s\n",
      "Epoch 93/100 \t loss = 1.1222188472747803 \t time = 14.74s\n",
      "Epoch 94/100 \t loss = 1.1173715591430664 \t time = 14.77s\n",
      "Epoch 95/100 \t loss = 1.1126049757003784 \t time = 14.77s\n",
      "Epoch 96/100 \t loss = 1.1079169511795044 \t time = 14.73s\n",
      "Epoch 97/100 \t loss = 1.1033055782318115 \t time = 14.76s\n",
      "Epoch 98/100 \t loss = 1.0987681150436401 \t time = 14.73s\n",
      "Epoch 99/100 \t loss = 1.0943026542663574 \t time = 14.78s\n",
      "Epoch 100/100 \t loss = 1.0899068117141724 \t time = 14.74s\n"
     ]
    }
   ],
   "source": [
    "# train using stochastic gradient descent\n",
    "\n",
    "# number of training examples\n",
    "N = len(targets_data)\n",
    "\n",
    "# learning rate\n",
    "lr = 10.\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "# batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# number of batches per epoch\n",
    "n_batches = N // batch_size\n",
    "\n",
    "# keep track of losses\n",
    "epoch_losses = []\n",
    "\n",
    "# training the network\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    # shuffle data\n",
    "    perm = np.random.permutation(N)\n",
    "    targets_epoch = targets_data[perm]\n",
    "    contexts_epoch = contexts_data[perm]\n",
    "    labels_epoch = labels_data[perm]\n",
    "\n",
    "    # decrease learning rate\n",
    "    if epoch == 20 or epoch == 50 or epoch == 75 or epoch == 90:\n",
    "        lr /= 2.\n",
    "\n",
    "    # losses in each epoch\n",
    "    losses = []\n",
    "    for batch in range(n_batches):\n",
    "        targets_batch = targets_epoch[batch*batch_size: (batch+1)*batch_size]\n",
    "        contexts_batch = contexts_epoch[batch*batch_size: (batch+1)*batch_size]\n",
    "        labels_batch = labels_epoch[batch*batch_size: (batch+1)*batch_size]\n",
    "\n",
    "        # calculate and save losses\n",
    "        loss_value, gradient = loss_value_and_grad(params, targets_batch, contexts_batch)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        params = [(V - lr*dV, U - lr*dU) for (V, U), (dV, dU) in zip(params, gradient)]\n",
    "\n",
    "    epoch_losses.append(np.mean(losses))\n",
    "\n",
    "    end_time = time.time()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} \\t loss = {np.mean(epoch_losses)} \\t time = {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_trained shape: (300, 14829)\n",
      "U_trained shape: (14829, 300)\n"
     ]
    }
   ],
   "source": [
    "# copy U and V\n",
    "V_trained = np.copy(params[0][0])\n",
    "U_trained = np.copy(params[0][1])\n",
    "\n",
    "# check dimensions of U and V, 100 epochs, lr = 1, batch_size = 500\n",
    "print(f'V_trained shape: {V_trained.shape}')\n",
    "print(f'U_trained shape: {U_trained.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate against test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "# define a function that takes in as input target and vocab length\n",
    "# outputs a one-hot vector, x_hot of dimension = (vocab_length,)\n",
    "def get_x_hot(target_idx, vocab_length):\n",
    "    x_hot = np.zeros(vocab_length, dtype=float)\n",
    "    x_hot[target_idx] = 1.0\n",
    "    return jnp.array(x_hot)\n",
    "\n",
    "# define softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "print(softmax(np.array([1, 2, 3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['public',\n",
       " 'loos',\n",
       " 'charlestown',\n",
       " 'â€“',\n",
       " 'historic',\n",
       " 'harbour',\n",
       " 'location',\n",
       " 'bbc',\n",
       " 'blockbuster',\n",
       " 'sunday',\n",
       " 'drama',\n",
       " 'poldark',\n",
       " 'filmed',\n",
       " 'smashed',\n",
       " 'auction',\n",
       " 'guide',\n",
       " 'price',\n",
       " 'Â£75000',\n",
       " 'Â£95000',\n",
       " 'sale']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see first 20 words in the vocab\n",
    "test_words = list(vocab.keys())[20:40]\n",
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('undermined', 0.8307438),\n",
       " ('disrepute', 0.8226104),\n",
       " ('reeks', 0.8185903),\n",
       " ('prohibits', 0.8088604),\n",
       " ('13', 0.80728334)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define cosine similarity scores between 2 word vectors\n",
    "def similarity_score(target_word_embedding, context_word_embedding):\n",
    "    return np.dot(target_word_embedding, context_word_embedding) / ((np.linalg.norm(target_word_embedding) * np.linalg.norm(context_word_embedding)))\n",
    "\n",
    "# define a function that find the most similar words to a given word\n",
    "def most_similar_words(word, V, n=5):\n",
    "    scores = []\n",
    "    target_word_idx = vocab[word]\n",
    "    for i in range(V.shape[1]):\n",
    "        if i == target_word_idx or inverse_vocab[i] == '<pad>':\n",
    "            continue\n",
    "        scores.append((inverse_vocab[i], similarity_score(V[:, target_word_idx], V[:, i])))\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return scores[:n]\n",
    "\n",
    "most_similar_words('public', V_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a forward pass through the skip-gram model\n",
    "\n",
    "# define the forward pass function\n",
    "def net(V, U, target_word_idx):\n",
    "    target_hot = get_x_hot(target_word_idx, len(vocab))\n",
    "    return softmax( U @ V @ target_hot )\n",
    "\n",
    "def predict(word, V, U):\n",
    "    target_word_idx = vocab[word]\n",
    "    y_hat = net(V, U, target_word_idx)\n",
    "    # y_hat is the probability distribution over the vocab\n",
    "    # select the top 5 words with the highest probability\n",
    "    top_5 = np.argsort(y_hat)[-10:][::-1]\n",
    "    top_5_words = [inverse_vocab[i] for i in top_5]\n",
    "    return top_5_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: contradictions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['emily',\n",
       " 'team',\n",
       " 'vettel',\n",
       " 'xavi',\n",
       " 'â€”',\n",
       " 'france',\n",
       " 'krejcir',\n",
       " 'sterlings',\n",
       " 'jean',\n",
       " 'arias']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select 1 word from vocab\n",
    "word = np.random.choice(list(vocab.keys()))\n",
    "y_hat = net(V_trained, U_trained, vocab[word])\n",
    "print(f'Word: {word}')\n",
    "predict(word, V_trained, U_trained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
