{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models\n",
    "\n",
    "**Outline**\n",
    "\n",
    "1. Read the top 2 word embeddings only\n",
    "2. Plot epoch_losses, train_accuracies, train_mses, test_accuracies, test_mses\n",
    "3. Using the best model, select 2 pairs of most similar words\n",
    "    * plot t-sne\n",
    "    * plot PCA\n",
    "4. Pass 2 target words through 1 forward pass of the skip-gram model\n",
    "    * select top 6 context words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Used | Num samples used | Num of Neg Samples | Train Accuracy | Test Accuracy | Train MSE | Test MSE |\n",
    "|:-----------|:-----------------|:-------------------|:---------------|:--------------|:----------|:---------|\n",
    "| SGNS_1 | 1M - 100 epochs, 10,000 iter | 5 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# import libraries relevant to plotting (t-SNE)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cosine similarity scores between 2 word vectors\n",
    "def similarity_score(target_word_embedding, context_word_embedding):\n",
    "    return np.dot(target_word_embedding, context_word_embedding) / (np.linalg.norm(target_word_embedding) * np.linalg.norm(context_word_embedding))\n",
    "\n",
    "# define a function that find the most similar words to a given word\n",
    "def most_similar_words(word, V, n=5):\n",
    "    scores = []\n",
    "    target_word_idx = vocab[word]\n",
    "    for i in range(V.shape[1]):\n",
    "        if i == target_word_idx or inverse_vocab[i] == '<pad>':\n",
    "            continue\n",
    "        scores.append((inverse_vocab[i], similarity_score(V[:, target_word_idx], V[:, i])))\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a forward pass through the skip-gram model\n",
    "# define function that takes in an index and vocab size and returns the one-hot encoding\n",
    "def getOneHot(index, vocab_size):\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[index] = 1\n",
    "    return onehot\n",
    "\n",
    "# define softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# define the forward pass function\n",
    "def net(V, U, target_word_idx):\n",
    "    target_hot = getOneHot(target_word_idx, len(vocab))\n",
    "    return softmax( U @ V @ target_hot )\n",
    "\n",
    "def predict(word, V, U):\n",
    "    target_word_idx = vocab[word]\n",
    "    y_hat = net(V, U, target_word_idx)\n",
    "    # y_hat is the probability distribution over the vocab\n",
    "    # select the top 5 words with the highest probability\n",
    "    top_5 = np.argsort(y_hat)[-5:][::-1]\n",
    "    top_5_words = [inverse_vocab[i] for i in top_5]\n",
    "    return top_5_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read vocab and inverse vocab json files from data/processed_data\n",
    "import json\n",
    "with open('./data/processed_data/vocab.json') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "with open('./data/processed_data/inverse_vocab.json') as f:\n",
    "    inverse_vocab = json.load(f)\n",
    "\n",
    "# read top 2 word embeddings\n",
    "    \n",
    "# read their corresponding results\n",
    "\n",
    "# check shape of V and U and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select n random words from the vocab list\n",
    "n = 5\n",
    "random_words = np.random.choice(list(vocab.keys()), n)\n",
    "print(f'Random words: {random_words}')\n",
    "\n",
    "# get the embedding vectors of the random words\n",
    "random_words_idx = [vocab[word] for word in random_words]\n",
    "\n",
    "# find the most similar word to the random words\n",
    "for i, word in enumerate(random_words):\n",
    "    print(f\"Most similar word to '{word}': {most_similar_words(random_words_idx[i], V_trained)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "V_trained_tsne = tsne.fit_transform(V_trained.T)\n",
    "\n",
    "# plot only selected words in the list\n",
    "target_words = ['produced', 'lie', 'chromecast', 'evolved', 'closed']\n",
    "similar_words = [most_similar_word(vocab[word], V_trained) for word in words]\n",
    "\n",
    "target_indices = [vocab[word] for word in words]\n",
    "similar_indices = [vocab[word] for word in similar_words]\n",
    "V_trained_tsne_target_words = V_trained_tsne[target_indices]\n",
    "V_trained_tsne_similar_words = V_trained_tsne[similar_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the t-sne graph, labelling the data point with the words\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(V_trained_tsne_target_words[:, 0], V_trained_tsne_target_words[:, 1], color='blue')\n",
    "plt.scatter(V_trained_tsne_similar_words[:, 0], V_trained_tsne_similar_words[:, 1], color='red')\n",
    "\n",
    "# label the data points with its corresponding words slightly above each data point\n",
    "for i, word in enumerate(words):\n",
    "    x, y = V_trained_tsne_target_words[i]\n",
    "    plt.text(x, y+0.1, word, fontsize=9)\n",
    "\n",
    "    x, y = V_trained_tsne_similar_words[i]\n",
    "    plt.text(x, y+0.1, similar_words[i], fontsize=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see first 20 words in the vocab\n",
    "test_words = list(vocab.keys())[20:40]\n",
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 1 word from vocab\n",
    "word = np.random.choice(list(vocab.keys()))\n",
    "y_hat = net(V_trained, U_trained, vocab[word])\n",
    "print(f'Word: {word}')\n",
    "predict(word, V_trained, U_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check for Quality of Embeddings**\n",
    "\n",
    "* Cosine Similarity Check\n",
    "    * Select a few words from the dictionary\n",
    "    * Find the most similar word (use cosine rule)\n",
    "\n",
    "* t-SNE plot\n",
    "    * e.g. king and queen vs cars and trucks\n",
    "    * expect king and queen to be closer together and cars and trucks to be closer together"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
