{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (13368, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A middle-school teacher in China has inked hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man convicted of killing the father and sist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avid rugby fan Prince Harry could barely watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Triple M Radio producer has been inundated w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Sally Forrest, an actress-dancer who graced th...\n",
       "1  A middle-school teacher in China has inked hun...\n",
       "2  A man convicted of killing the father and sist...\n",
       "3  Avid rugby fan Prince Harry could barely watch...\n",
       "4  A Triple M Radio producer has been inundated w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('./data/raw data/raw_data.csv', header=0, names=['text'], usecols=[1])\n",
    "print(f'Data Shape: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punctuations = string.punctuation\n",
    "def remove_punctuation(txt):\n",
    "    for char in punctuations:\n",
    "        if char in txt:\n",
    "            txt = txt.replace(char, \"\")\n",
    "    return txt\n",
    "\n",
    "# change to lower caps\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# remove punctuations\n",
    "data['text'] = data['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sally forrest an actressdancer who graced the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a middleschool teacher in china has inked hund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a man convicted of killing the father and sist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avid rugby fan prince harry could barely watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a triple m radio producer has been inundated w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  sally forrest an actressdancer who graced the ...\n",
       "1  a middleschool teacher in china has inked hund...\n",
       "2  a man convicted of killing the father and sist...\n",
       "3  avid rugby fan prince harry could barely watch...\n",
       "4  a triple m radio producer has been inundated w..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sally forrest an actressdancer who graced the silver screen throughout the 40s and 50s in mgm musicals and films such as the 1956 noir while the city sleeps died on march 15 at her home in beverly hills california forrest whose birth name was katherine feeney was 86 and had long battled cancer her publicist judith goffin announced the news thursday scroll down for video  actress sally forrest was in the 1951 ida lupinodirected film hard fast and beautiful left and the 1956 fritz lang movie while the city sleeps a san diego native forrest became a protege of hollywood trailblazer ida lupino who cast her in starring roles in films including the critical and commercial success not wanted never fear and hard fast and beautiful some of forrests other film credits included bannerline son of sinbad and excuse my dust according to her imdbÂ page the page also indicates forrest was in multiple climax and rawhide television episodes forrest appeared as herself in an episode of the ed sullivan show and three episodes of the dinah shore chevy show her imdb page says she also starred in a broadway production of the seven year itch city news service reported that other stage credits included as you like it no no nanette and damn yankees forrest married writerproducer milo frank in 1951 he died in 2004 she is survived by her niece sharon durham and nephews michael and mark feeney career a san diego native forrest became a protege of hollywood trailblazer ida lupino who cast her in starring roles in films \n"
     ]
    }
   ],
   "source": [
    "# print first entry\n",
    "print(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows of data: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [a, host, of, celebrities, including, kelly, o...\n",
       "1    [tony, pulis, called, for, the, introduction, ...\n",
       "2    [to, kill, a, mockingbird, author, harper, lee...\n",
       "3    [university, of, houston, officials, have, sus...\n",
       "4    [fresh, security, fears, have, been, raised, i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each row into list of words\n",
    "data_lst = data['text'].apply(lambda txt: txt.split(\" \"))\n",
    "\n",
    "# select number of rows to be used as training data\n",
    "nrows = 200\n",
    "random_indices = np.random.randint(low=0, high=len(data_lst), size=nrows)\n",
    "data_lst = data_lst[random_indices].reset_index(drop=True)\n",
    "\n",
    "print(f'Number of rows of data: {len(data_lst)}')\n",
    "data_lst[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows of training data: 180\n",
      "Number of rows of test data: 20\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "n_test = 20\n",
    "test_indices = np.random.randint(low=0, high=len(data_lst), size=n_test)\n",
    "train_indices = [i for i in range(len(data_lst)) if i not in test_indices]\n",
    "\n",
    "train_data = data_lst[train_indices]\n",
    "test_data = data_lst[test_indices]\n",
    "\n",
    "print(f'Number of rows of training data: {len(train_data)}')\n",
    "print(f'Number of rows of test data: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 15063\n"
     ]
    }
   ],
   "source": [
    "# vocab dict\n",
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "for line in data_lst:\n",
    "    for word in line:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse_vocab dict\n",
    "inverse_vocab = {}\n",
    "for word, index in vocab.items():\n",
    "    inverse_vocab[index] = word\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab and inverse_vocab in processed_data folder\n",
    "import json\n",
    "with open('./data/processed_data/vocab.json', 'w') as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "with open('./data/processed_data/inverse_vocab.json', 'w') as f:\n",
    "    json.dump(inverse_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14119\n"
     ]
    }
   ],
   "source": [
    "# vocab_train dict\n",
    "vocab_train, index = {}, 1\n",
    "vocab_train['<pad>'] = 0\n",
    "for line in train_data:\n",
    "    for word in line:\n",
    "        if word not in vocab_train:\n",
    "            vocab_train[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse_vocab dict\n",
    "inverse_vocab_train = {}\n",
    "for word, index in vocab_train.items():\n",
    "    inverse_vocab_train[index] = word\n",
    "\n",
    "print(f'Vocab size: {len(vocab_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab and inverse_vocab in processed_data folder\n",
    "import json\n",
    "with open('./data/processed_data/vocab_train.json', 'w') as f:\n",
    "    json.dump(vocab_train, f)\n",
    "\n",
    "with open('./data/processed_data/inverse_vocab_train.json', 'w') as f:\n",
    "    json.dump(inverse_vocab_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3382\n"
     ]
    }
   ],
   "source": [
    "# vocab_test dict\n",
    "vocab_test, index = {}, 1\n",
    "vocab_test['<pad>'] = 0\n",
    "for line in test_data:\n",
    "    for word in line:\n",
    "        if word not in vocab_test:\n",
    "            vocab_test[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse_vocab dict\n",
    "inverse_vocab_test = {}\n",
    "for word, index in vocab_test.items():\n",
    "    inverse_vocab_test[index] = word\n",
    "\n",
    "print(f'Vocab size: {len(vocab_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab and inverse_vocab in processed_data folder\n",
    "import json\n",
    "with open('./data/processed_data/vocab_test.json', 'w') as f:\n",
    "    json.dump(vocab_test, f)\n",
    "\n",
    "with open('./data/processed_data/inverse_vocab_test.json', 'w') as f:\n",
    "    json.dump(inverse_vocab_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train sequences\n",
    "train_sequences = []\n",
    "for line in train_data:\n",
    "    vectorized_line = [vocab_train[word] for word in line]\n",
    "    train_sequences.append(vectorized_line)\n",
    "\n",
    "# test sequences\n",
    "test_sequences = []\n",
    "for line in test_data:\n",
    "    vectorized_line = [vocab_test[word] for word in line]\n",
    "    test_sequences.append(vectorized_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate samples\n",
    "def generate_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0,\n",
    "          shuffle=True)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.reshape(tf.constant([context_word], dtype=\"int64\"), (1,1))\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 180/180 [01:50<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape: (227036,)\n",
      "contexts shape: (227036, 6)\n",
      "labels shape: (227036, 6)\n"
     ]
    }
   ],
   "source": [
    "# generate training data\n",
    "window_size = 3\n",
    "num_ns = 5\n",
    "vocab_size = len(vocab_train)\n",
    "seed = 4212\n",
    "\n",
    "targets, contexts, labels = generate_data(sequences=train_sequences,\n",
    "                                          window_size=window_size,\n",
    "                                          num_ns=num_ns,\n",
    "                                          vocab_size=vocab_size,\n",
    "                                          seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f'targets shape: {targets.shape}')\n",
    "print(f'contexts shape: {contexts.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 20/20 [00:08<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets_test shape: (16876,)\n",
      "contexts_test shape: (16876, 6)\n",
      "labels_test shape: (16876, 6)\n"
     ]
    }
   ],
   "source": [
    "# generate testing data\n",
    "window_size = 3\n",
    "num_ns = 5\n",
    "vocab_size = len(vocab_test)\n",
    "seed = 4212\n",
    "\n",
    "targets_test, contexts_test, labels_test = generate_data(sequences=test_sequences,\n",
    "                                                         window_size=window_size,\n",
    "                                                         vocab_size=vocab_size,\n",
    "                                                         num_ns=num_ns,\n",
    "                                                         seed=seed)\n",
    "\n",
    "targets_test = np.array(targets_test)\n",
    "contexts_test = np.array(contexts_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "print(f'targets_test shape: {targets_test.shape}')\n",
    "print(f'contexts_test shape: {contexts_test.shape}')\n",
    "print(f'labels_test shape: {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training and testing data in data/processed/train and data/processed/test\n",
    "np.save('./data/processed_data/train/num_ns_5/targets.npy', targets)\n",
    "np.save('./data/processed_data/train/num_ns_5/contexts.npy', contexts)\n",
    "np.save('./data/processed_data/train/num_ns_5/labels.npy', labels)\n",
    "\n",
    "np.save('./data/processed_data/test/num_ns_5/targets.npy', targets_test)\n",
    "np.save('./data/processed_data/test/num_ns_5/contexts.npy', contexts_test)\n",
    "np.save('./data/processed_data/test/num_ns_5/labels.npy', labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check on quality of training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 196\n",
      "target_word     : etc\n",
      "context_indices : [ 195   38 1524 1534 1029  139]\n",
      "context_words   : ['via', 'commend', 'event', 'moneymaking', 'becoming', 'lowkey']\n",
      "label           : [1 0 0 0 0 0]\n",
      "target  : 196\n",
      "context : [ 195   38 1524 1534 1029  139]\n",
      "label   : [1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "print(f\"target_index    : {targets[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab_train[targets[0]]}\")\n",
    "print(f\"context_indices : {contexts[0]}\")\n",
    "print(f\"context_words   : {[inverse_vocab_train[c] for c in contexts[0]]}\")\n",
    "print(f\"label           : {labels[0]}\")\n",
    "\n",
    "print(\"target  :\", targets[0])\n",
    "print(\"context :\", contexts[0])\n",
    "print(\"label   :\", labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 64\n",
      "target_word     : end\n",
      "context_indices : [ 29  21 508 511 364  66]\n",
      "context_words   : ['of', 'boxing', 'far', 'serious', 'says', 'in']\n",
      "label           : [1 0 0 0 0 0]\n",
      "target  : 196\n",
      "context : [ 195   38 1524 1534 1029  139]\n",
      "label   : [1 0 0 0 0 0]\n",
      "target  : 64\n",
      "context : [ 29  21 508 511 364  66]\n",
      "label   : [1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "print(f\"target_index    : {targets_test[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab_test[targets_test[0]]}\")\n",
    "print(f\"context_indices : {contexts_test[0]}\")\n",
    "print(f\"context_words   : {[inverse_vocab_test[c] for c in contexts_test[0]]}\")\n",
    "print(f\"label           : {labels_test[0]}\")\n",
    "\n",
    "print(\"target  :\", targets[0])\n",
    "print(\"context :\", contexts[0])\n",
    "print(\"label   :\", labels[0])\n",
    "\n",
    "print(\"target  :\", targets_test[0])\n",
    "print(\"context :\", contexts_test[0])\n",
    "print(\"label   :\", labels_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
