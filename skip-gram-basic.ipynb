{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram (Not Optimized)\n",
    "\n",
    "#### Outline\n",
    "1. Data Preprocessing\n",
    "2. Generate Training and Testing Data\n",
    "3. Define the Skip-Gram Model\n",
    "4. SGD\n",
    "5. Evaluation of Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (13368, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A middle-school teacher in China has inked hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man convicted of killing the father and sist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avid rugby fan Prince Harry could barely watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Triple M Radio producer has been inundated w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Sally Forrest, an actress-dancer who graced th...\n",
       "1  A middle-school teacher in China has inked hun...\n",
       "2  A man convicted of killing the father and sist...\n",
       "3  Avid rugby fan Prince Harry could barely watch...\n",
       "4  A Triple M Radio producer has been inundated w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Data\n",
    "data = pd.read_csv('./data/raw data/raw_data.csv', header=0, names=['text'], usecols=[1])\n",
    "print(f'Data Shape: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punctuations = string.punctuation\n",
    "def remove_punctuation(txt):\n",
    "    for char in punctuations:\n",
    "        if char in txt:\n",
    "            txt = txt.replace(char, \"\")\n",
    "    return txt\n",
    "\n",
    "# change to lower caps\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# remove punctuations\n",
    "data['text'] = data['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# read stopwords from data/raw data/stopwords.txt\n",
    "stop_words = []\n",
    "with open('./data/raw data/stopwords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip())\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    txt = [word for word in txt.split() if word not in stop_words]\n",
    "    return ' '.join(txt)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows of data: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [gary, neville, spent, friday, night, manchest...\n",
       "1    [passionate, conservation, chinas, yunnan, pro...\n",
       "2    [sight, world, wince, sympathy, chinese, build...\n",
       "3    [buzz, aldrin, walk, moon, worlds, monuments, ...\n",
       "4    [australians, splash, study, swimming, common,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each row into list of words\n",
    "data_lst = data['text'].apply(lambda txt: txt.split(\" \"))\n",
    "\n",
    "# select number of rows to be used as training data\n",
    "nrows = 200\n",
    "random_indices = np.random.randint(low=0, high=len(data_lst), size=nrows)\n",
    "data_lst = data_lst[random_indices].reset_index(drop=True)\n",
    "\n",
    "print(f'Number of rows of data: {len(data_lst)}')\n",
    "data_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14233\n"
     ]
    }
   ],
   "source": [
    "# vocab dict\n",
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "for line in data_lst:\n",
    "    for word in line:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse_vocab dict\n",
    "inverse_vocab = {}\n",
    "for word, index in vocab.items():\n",
    "    inverse_vocab[index] = word\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "sequences = []\n",
    "for line in data_lst:\n",
    "    vectorized_line = [vocab[word] for word in line]\n",
    "    sequences.append(vectorized_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# choose 20 random sequences\n",
    "ntest = 20\n",
    "test_indices = np.random.randint(low=0, high=len(sequences), size=ntest)\n",
    "test_sequences = [sequences[i] for i in test_indices]\n",
    "train_sequences = [sequences[i] for i in range(len(sequences)) if i not in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gary',\n",
       " 'neville',\n",
       " 'spent',\n",
       " 'friday',\n",
       " 'night',\n",
       " 'manchesters',\n",
       " 'finest',\n",
       " 'england',\n",
       " 'assistant',\n",
       " 'watched',\n",
       " 'charlatans',\n",
       " 'albert',\n",
       " 'hall',\n",
       " 'former',\n",
       " 'manchester',\n",
       " 'united',\n",
       " 'defender',\n",
       " 'smiles',\n",
       " 'time',\n",
       " 'charlatans',\n",
       " 'lead',\n",
       " 'singer',\n",
       " 'tim',\n",
       " 'burgess',\n",
       " 'stone',\n",
       " 'roses',\n",
       " 'legend',\n",
       " 'mani',\n",
       " 'courteeners',\n",
       " 'frontman',\n",
       " 'liam',\n",
       " 'fray',\n",
       " 'ahead',\n",
       " 'gig',\n",
       " 'tweeting',\n",
       " 'image',\n",
       " 'mancunians',\n",
       " 'twitter',\n",
       " 'burgess',\n",
       " 'joked',\n",
       " '5',\n",
       " 'aside',\n",
       " 'team',\n",
       " 'gary',\n",
       " 'neville',\n",
       " 'left',\n",
       " 'poses',\n",
       " 'tim',\n",
       " 'burgess',\n",
       " 'lead',\n",
       " 'singer',\n",
       " 'charlatans',\n",
       " 'left',\n",
       " 'mani',\n",
       " 'liam',\n",
       " 'fray',\n",
       " 'former',\n",
       " 'manchester',\n",
       " 'united',\n",
       " 'england',\n",
       " 'defender',\n",
       " 'smiles',\n",
       " 'ahead',\n",
       " 'charlatans',\n",
       " 'gig',\n",
       " 'neville',\n",
       " 'shared',\n",
       " 'snaps',\n",
       " 'seat',\n",
       " 'gig',\n",
       " 'seemingly',\n",
       " 'fan',\n",
       " 'madchester',\n",
       " 'scene',\n",
       " 'seemingly',\n",
       " 'fan',\n",
       " 'madchester',\n",
       " 'music',\n",
       " 'scene',\n",
       " 'height',\n",
       " 'late',\n",
       " '80s',\n",
       " '90s',\n",
       " '40yearold',\n",
       " 'posted',\n",
       " 'snaps',\n",
       " 'gig',\n",
       " 'seat',\n",
       " 'neville',\n",
       " 'posed',\n",
       " 'burgess',\n",
       " 'signed',\n",
       " 'setlist',\n",
       " 'saying',\n",
       " 'twitter',\n",
       " 'amazing',\n",
       " 'night',\n",
       " 'tim',\n",
       " 'burgess',\n",
       " 'charlatans',\n",
       " 'december',\n",
       " 'neville',\n",
       " 'shared',\n",
       " 'image',\n",
       " 'courteeners',\n",
       " 'star',\n",
       " 'fray',\n",
       " 'wished',\n",
       " 'beloved',\n",
       " 'manchester',\n",
       " 'merry',\n",
       " 'christmas',\n",
       " 'burgess',\n",
       " 'meanwhile',\n",
       " 'chuffed',\n",
       " 'bands',\n",
       " 'song',\n",
       " 'home',\n",
       " 'baby',\n",
       " 'charlatans',\n",
       " 'album',\n",
       " 'modern',\n",
       " 'nature',\n",
       " 'included',\n",
       " 'trafford',\n",
       " 'playlist',\n",
       " 'manchester',\n",
       " 'uniteds',\n",
       " '20',\n",
       " 'victory',\n",
       " 'sunderland',\n",
       " 'week',\n",
       " 'stone',\n",
       " 'roses',\n",
       " 'bassist',\n",
       " 'mani',\n",
       " 'real',\n",
       " 'name',\n",
       " 'gary',\n",
       " 'mounfield',\n",
       " 'spell',\n",
       " 'primal',\n",
       " 'scream',\n",
       " 'staunch',\n",
       " 'manchester',\n",
       " 'united',\n",
       " 'fan',\n",
       " 'nou',\n",
       " 'camp',\n",
       " 'red',\n",
       " 'devils',\n",
       " 'complete',\n",
       " 'treble',\n",
       " 'lift',\n",
       " 'champions',\n",
       " 'league',\n",
       " 'trophy',\n",
       " '1999',\n",
       " 'stone',\n",
       " 'roses',\n",
       " 'legend',\n",
       " 'mani',\n",
       " 'witnessed',\n",
       " 'neville',\n",
       " 'lifting',\n",
       " 'champions',\n",
       " 'league',\n",
       " 'trophy',\n",
       " 'nou',\n",
       " 'camp',\n",
       " '1999']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inverse_vocab[i] for i in train_sequences[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['british',\n",
       " 'game',\n",
       " 'thrones',\n",
       " 'fans',\n",
       " 'paying',\n",
       " '£60',\n",
       " 'shoppers',\n",
       " 'hands',\n",
       " 'products',\n",
       " 'research',\n",
       " 'comparing',\n",
       " 'price',\n",
       " 'merchandise',\n",
       " 'hbos',\n",
       " 'online',\n",
       " 'store',\n",
       " 'found',\n",
       " 'huge',\n",
       " 'markups',\n",
       " 'stretch',\n",
       " 'jewellery',\n",
       " 'clothing',\n",
       " 'toys',\n",
       " 'sterling',\n",
       " 'silver',\n",
       " 'targaryen',\n",
       " 'pendant',\n",
       " 'costs',\n",
       " '£6299',\n",
       " 'uk',\n",
       " '£3945',\n",
       " 'pond',\n",
       " 'british',\n",
       " 'game',\n",
       " 'thrones',\n",
       " 'fans',\n",
       " 'paying',\n",
       " 'shoppers',\n",
       " 'game',\n",
       " 'thrones',\n",
       " 'merchandise',\n",
       " 'left',\n",
       " 'targaryen',\n",
       " 'ring',\n",
       " 'costs',\n",
       " '£2799',\n",
       " 'uk',\n",
       " 'equivalent',\n",
       " '£2340',\n",
       " 'america',\n",
       " 'sterling',\n",
       " 'silver',\n",
       " 'targaryen',\n",
       " 'pendant',\n",
       " 'costs',\n",
       " '£6299',\n",
       " 'uk',\n",
       " '£3945',\n",
       " 'pond',\n",
       " 'similarly',\n",
       " 'sterling',\n",
       " 'silver',\n",
       " 'dragon',\n",
       " 'egg',\n",
       " 'pendant',\n",
       " 'sets',\n",
       " 'british',\n",
       " 'fans',\n",
       " '£11799',\n",
       " 'americans',\n",
       " 'paying',\n",
       " '£5835',\n",
       " 'bookend',\n",
       " 'shape',\n",
       " 'iron',\n",
       " 'throne',\n",
       " 'nearly',\n",
       " '£20',\n",
       " 'expensive',\n",
       " 'uk',\n",
       " 'letter',\n",
       " 'opener',\n",
       " '£895',\n",
       " 'shoppers',\n",
       " 'buying',\n",
       " 'figurines',\n",
       " 'fork',\n",
       " '£462',\n",
       " 'uk',\n",
       " 'increase',\n",
       " '25',\n",
       " 'cent',\n",
       " 'uk',\n",
       " 'shoppers',\n",
       " 'pay',\n",
       " '13',\n",
       " 'cent',\n",
       " 'european',\n",
       " 'counterparts',\n",
       " 'sun',\n",
       " 'reported',\n",
       " 'rob',\n",
       " 'davies',\n",
       " '21yearold',\n",
       " 'fan',\n",
       " 'fans',\n",
       " 'treated',\n",
       " 'equally',\n",
       " 'fan',\n",
       " 'jess',\n",
       " 'lilley',\n",
       " 'am',\n",
       " 'outraged',\n",
       " 'produced',\n",
       " 'americans',\n",
       " 'shoppers',\n",
       " 'buying',\n",
       " 'figurines',\n",
       " 'daenerys',\n",
       " 'targaryen',\n",
       " 'left',\n",
       " 'doll',\n",
       " 'fork',\n",
       " '£462',\n",
       " 'uk',\n",
       " 'increase',\n",
       " '25',\n",
       " 'cent',\n",
       " '£1337',\n",
       " 'price',\n",
       " 'tag',\n",
       " 'samwell',\n",
       " 'tarly',\n",
       " 'toy',\n",
       " 'costs',\n",
       " '£899',\n",
       " 'uk',\n",
       " '£802',\n",
       " 'buy',\n",
       " 'hbos',\n",
       " 'american',\n",
       " 'site',\n",
       " 'comes',\n",
       " 'fifth',\n",
       " 'series',\n",
       " 'hbo',\n",
       " 'premiered',\n",
       " 'london',\n",
       " 'wednesday',\n",
       " 'american',\n",
       " 'fantasy',\n",
       " 'drama',\n",
       " 'shown',\n",
       " 'sky',\n",
       " 'britain',\n",
       " 'attracted',\n",
       " 'record',\n",
       " 'viewers',\n",
       " 'channel',\n",
       " 'created',\n",
       " 'lucrative',\n",
       " 'opportunity',\n",
       " 'merchandise',\n",
       " 'topend',\n",
       " 'products',\n",
       " 'sold',\n",
       " 'hbos',\n",
       " 'site',\n",
       " 'include',\n",
       " 'lifesize',\n",
       " 'replica',\n",
       " 'iron',\n",
       " 'throne',\n",
       " '30000',\n",
       " '£20000',\n",
       " 'successive',\n",
       " 'british',\n",
       " 'governments',\n",
       " 'complained',\n",
       " 'global',\n",
       " 'firms',\n",
       " 'consistently',\n",
       " 'charging',\n",
       " 'british',\n",
       " 'consumers',\n",
       " 'figure',\n",
       " 'jaime',\n",
       " 'lannister',\n",
       " 'left',\n",
       " '£2099',\n",
       " 'uk',\n",
       " '£1671',\n",
       " 'house',\n",
       " 'stark',\n",
       " 'direwolf',\n",
       " 'pillow',\n",
       " 'nearly',\n",
       " 'half',\n",
       " 'price',\n",
       " 'america',\n",
       " '£2006',\n",
       " 'compared',\n",
       " '£3499',\n",
       " 'bookend',\n",
       " 'shape',\n",
       " 'iron',\n",
       " 'throne',\n",
       " 'nearly',\n",
       " '£20',\n",
       " 'expensive',\n",
       " 'uk',\n",
       " 'letter',\n",
       " 'opener',\n",
       " '£895',\n",
       " 'left',\n",
       " 'consumer',\n",
       " 'survey',\n",
       " 'found',\n",
       " 'price',\n",
       " 'difference',\n",
       " 'particularly',\n",
       " 'prevalent',\n",
       " 'technology',\n",
       " 'amazon',\n",
       " 'prime',\n",
       " '£20',\n",
       " 'cheaper',\n",
       " '£59',\n",
       " 'survey',\n",
       " 'found',\n",
       " '13inch',\n",
       " 'apple',\n",
       " 'macbook',\n",
       " 'pro',\n",
       " 'cost',\n",
       " '£1144',\n",
       " 'instead',\n",
       " '£1499',\n",
       " 'uk',\n",
       " 'price',\n",
       " 'difference',\n",
       " 'due',\n",
       " 'uk',\n",
       " 'vat',\n",
       " 'rate',\n",
       " 'hbo',\n",
       " 'spokesman',\n",
       " 'majority',\n",
       " 'licensees',\n",
       " 'outside',\n",
       " 'europe',\n",
       " 'creates',\n",
       " 'additional',\n",
       " 'layers',\n",
       " 'distribution',\n",
       " 'importation',\n",
       " 'costs',\n",
       " 'american',\n",
       " 'fantasy',\n",
       " 'drama',\n",
       " 'shown',\n",
       " 'sky',\n",
       " 'britain',\n",
       " 'attracted',\n",
       " 'record',\n",
       " 'viewers',\n",
       " 'hbo',\n",
       " 'created',\n",
       " 'lucrative',\n",
       " 'opportunity',\n",
       " 'merchandise',\n",
       " 'jon',\n",
       " 'snow',\n",
       " 'pictured',\n",
       " 'series']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inverse_vocab[i] for i in test_sequences[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate training and testing data\n",
    "def generate_data(sequences, window_size):\n",
    "    targets, contexts = [], []\n",
    "    for sequence in tqdm(sequences):\n",
    "        # for each sentence\n",
    "        for center_word_pos in range(len(sequence)):\n",
    "            # add to targets\n",
    "            targets.append(sequence[center_word_pos])\n",
    "\n",
    "            context = []\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make sure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sequence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                context_word_idx = sequence[context_word_pos]\n",
    "                context.append(context_word_idx)\n",
    "\n",
    "            # if length of context < 2*window_size\n",
    "            # pad context with None until length is 2 * window_size\n",
    "            if len(context) < 2 * window_size:\n",
    "                pad_length = (2 * window_size) - len(context)\n",
    "                for i in range(pad_length):\n",
    "                    context.append(None)\n",
    "\n",
    "            # add to contexts\n",
    "            context = np.array(context)#; print(context)\n",
    "            contexts.append(context)\n",
    "\n",
    "    targets = np.array(targets)\n",
    "    contexts = np.array(contexts)\n",
    "\n",
    "    return targets, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/183 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:00<00:00, 483.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape: (58804,)\n",
      "contexts shape: (58804, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 518.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_targets shape: (6341,)\n",
      "test_contexts shape: (6341, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate training data\n",
    "targets, contexts = generate_data(train_sequences, 5)\n",
    "\n",
    "print(f'targets shape: {targets.shape}')\n",
    "print(f'contexts shape: {contexts.shape}')\n",
    "\n",
    "\n",
    "# generate testing data\n",
    "test_targets, test_contexts = generate_data(test_sequences, 5)\n",
    "\n",
    "print(f'test_targets shape: {test_targets.shape}')\n",
    "print(f'test_contexts shape: {test_contexts.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print a few examples of training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 1\n",
      "target_word     : gary\n",
      "context_indices : [2 3 4 5 6 None None None None None]\n",
      "context_words   : ['neville']\n",
      "context_words   : ['spent']\n",
      "context_words   : ['friday']\n",
      "context_words   : ['night']\n",
      "context_words   : ['manchesters']\n",
      "target  : 1\n",
      "context : [2 3 4 5 6 None None None None None]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "index = 0\n",
    "print(f\"target_index    : {targets[index]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets[index]]}\")\n",
    "print(f\"context_indices : {contexts[index]}\")\n",
    "for c in contexts[index]:\n",
    "    if c == None:\n",
    "        continue\n",
    "    print(f\"context_words   : {[inverse_vocab[c]]}\")\n",
    "\n",
    "print(\"target  :\", targets[index])\n",
    "print(\"context :\", contexts[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 1528\n",
      "target_word     : british\n",
      "context_indices : [2447 11176 826 6635 11177 None None None None None]\n",
      "context_words   : ['game']\n",
      "context_words   : ['thrones']\n",
      "context_words   : ['fans']\n",
      "context_words   : ['paying']\n",
      "context_words   : ['£60']\n",
      "target  : 1528\n",
      "context : [2447 11176 826 6635 11177 None None None None None]\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "index = 0\n",
    "print(f\"target_index    : {test_targets[index]}\")\n",
    "print(f\"target_word     : {inverse_vocab[test_targets[index]]}\")\n",
    "print(f\"context_indices : {test_contexts[index]}\")\n",
    "for c in test_contexts[index]:\n",
    "    if c == None:\n",
    "        continue\n",
    "    print(f\"context_words   : {[inverse_vocab[c]]}\")\n",
    "\n",
    "print(\"target  :\", test_targets[index])\n",
    "print(\"context :\", test_contexts[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function that takes in as input target and vocab length\n",
    "# outputs a one-hot vector, x_hot of dimension = (vocab_length,)\n",
    "def get_x_hot(target_idx, vocab_length):\n",
    "    x_hot = np.zeros(vocab_length)\n",
    "    x_hot[target_idx] = 1.\n",
    "    return x_hot\n",
    "# get_x_hot(3, 10)\n",
    "\n",
    "def get_y_true(context_idxs, vocab_length):\n",
    "    y_true = np.zeros(vocab_length)\n",
    "    for i in context_idxs:\n",
    "        y_true[i] = 1.\n",
    "    return y_true\n",
    "# get_y_true(np.array([1, 2, 3]), 10)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "# softmax(np.array[1,2,3])\n",
    "\n",
    "# define a forward pass in the skip gram model \n",
    "def net(target_hot, V, U):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    x_hot is one-hot vector, dimensions = (|v| x 1)\n",
    "    V: input embedding matrix, dimension = (n x |v|)\n",
    "    U: output embedding matrix, dimension = (|v| x n)\n",
    "        |v| = vocab size\n",
    "        n = no. of embedding dimensions\n",
    "    Output\n",
    "    z: score vector of probability distribution, dimension = (|v| x 1)\n",
    "    \"\"\"\n",
    "    return softmax( U @ V @ target_hot )\n",
    "\n",
    "def local_loss(y_hat, y_true):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    y_hat: predicted vector from 1 pass through the network, dimension = (|v| x 1)\n",
    "    y_true: actual vector from get_y_true, dimension = (|v| x 1)\n",
    "\n",
    "    Output\n",
    "    loss_value: real number\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
