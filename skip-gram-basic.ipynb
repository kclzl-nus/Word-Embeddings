{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram (Not Optimized)\n",
    "\n",
    "#### Outline\n",
    "1. Data Preprocessing\n",
    "2. Generate Training and Testing Data\n",
    "3. Define the Skip-Gram Model\n",
    "4. SGD\n",
    "5. Evaluation of Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (13368, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A middle-school teacher in China has inked hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man convicted of killing the father and sist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avid rugby fan Prince Harry could barely watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Triple M Radio producer has been inundated w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Sally Forrest, an actress-dancer who graced th...\n",
       "1  A middle-school teacher in China has inked hun...\n",
       "2  A man convicted of killing the father and sist...\n",
       "3  Avid rugby fan Prince Harry could barely watch...\n",
       "4  A Triple M Radio producer has been inundated w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Data\n",
    "data = pd.read_csv('./data/raw data/raw_data.csv', header=0, names=['text'], usecols=[1])\n",
    "print(f'Data Shape: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punctuations = string.punctuation\n",
    "def remove_punctuation(txt):\n",
    "    for char in punctuations:\n",
    "        if char in txt:\n",
    "            txt = txt.replace(char, \"\")\n",
    "    return txt\n",
    "\n",
    "# change to lower caps\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# remove punctuations\n",
    "data['text'] = data['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# read stopwords from data/raw data/stopwords.txt\n",
    "stop_words = []\n",
    "with open('./data/raw data/stopwords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip())\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    txt = [word for word in txt.split() if word not in stop_words]\n",
    "    return ' '.join(txt)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows of data: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [school, children, country, watching, tomorrow...\n",
       "1    [oh, beating, heart, standing, gravel, outside...\n",
       "2    [former, guard, rikers, island, convicted, smu...\n",
       "3    [move, kim, kardashian, ideal, female, body, s...\n",
       "4    [trevor, noah, host, daily, spoken, defense, f...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each row into list of words\n",
    "data_lst = data['text'].apply(lambda txt: txt.split(\" \"))\n",
    "\n",
    "# select number of rows to be used as training data\n",
    "nrows = 200\n",
    "random_indices = np.random.randint(low=0, high=len(data_lst), size=nrows)\n",
    "data_lst = data_lst[random_indices].reset_index(drop=True)\n",
    "\n",
    "print(f'Number of rows of data: {len(data_lst)}')\n",
    "data_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14800\n"
     ]
    }
   ],
   "source": [
    "# vocab dict\n",
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "for line in data_lst:\n",
    "    for word in line:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse_vocab dict\n",
    "inverse_vocab = {}\n",
    "for word, index in vocab.items():\n",
    "    inverse_vocab[index] = word\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "sequences = []\n",
    "for line in data_lst:\n",
    "    vectorized_line = [vocab[word] for word in line]\n",
    "    sequences.append(vectorized_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# choose 20 random sequences\n",
    "ntest = 20\n",
    "test_indices = np.random.randint(low=0, high=len(sequences), size=ntest)\n",
    "test_sequences = [sequences[i] for i in test_indices]\n",
    "train_sequences = [sequences[i] for i in range(len(sequences)) if i not in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[inverse_vocab[i] for i in train_sequences[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[inverse_vocab[i] for i in test_sequences[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate training and testing data\n",
    "def generate_data(sequences, window_size):\n",
    "    targets, contexts = [], []\n",
    "    for sequence in tqdm(sequences):\n",
    "        # for each sentence\n",
    "        for center_word_pos in range(len(sequence)):\n",
    "            # add to targets\n",
    "            targets.append(sequence[center_word_pos])\n",
    "\n",
    "            context = []\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make sure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(sequence) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                context_word_idx = sequence[context_word_pos]\n",
    "                context.append(context_word_idx)\n",
    "\n",
    "            # if length of context < 2*window_size\n",
    "            # pad context with None until length is 2 * window_size\n",
    "            if len(context) < 2 * window_size:\n",
    "                pad_length = (2 * window_size) - len(context)\n",
    "                for i in range(pad_length):\n",
    "                    context.append(-1)\n",
    "\n",
    "            # add to contexts\n",
    "            context = np.array(context)#; print(context)\n",
    "            contexts.append(context)\n",
    "\n",
    "    targets = np.array(targets)\n",
    "    contexts = np.array(contexts)\n",
    "\n",
    "    return targets, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 49/182 [00:00<00:00, 480.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:00<00:00, 483.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape: (59569,)\n",
      "contexts shape: (59569, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 433.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_targets shape: (7452,)\n",
      "test_contexts shape: (7452, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate training data\n",
    "targets, contexts = generate_data(train_sequences, 5)\n",
    "\n",
    "print(f'targets shape: {targets.shape}')\n",
    "print(f'contexts shape: {contexts.shape}')\n",
    "\n",
    "\n",
    "# generate testing data\n",
    "test_targets, test_contexts = generate_data(test_sequences, 5)\n",
    "\n",
    "print(f'test_targets shape: {test_targets.shape}')\n",
    "print(f'test_contexts shape: {test_contexts.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print a few examples of training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 1\n",
      "target_word     : school\n",
      "context_indices : [ 2  3  4  5  6 -1 -1 -1 -1 -1]\n",
      "context_words   : ['children']\n",
      "context_words   : ['country']\n",
      "context_words   : ['watching']\n",
      "context_words   : ['tomorrows']\n",
      "context_words   : ['onceinadecade']\n",
      "target  : 1\n",
      "context : [ 2  3  4  5  6 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "index = 0\n",
    "print(f\"target_index    : {targets[index]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets[index]]}\")\n",
    "print(f\"context_indices : {contexts[index]}\")\n",
    "for c in contexts[index]:\n",
    "    if c == -1:\n",
    "        continue\n",
    "    print(f\"context_words   : {[inverse_vocab[c]]}\")\n",
    "\n",
    "print(\"target  :\", targets[index])\n",
    "print(\"context :\", contexts[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 11403\n",
      "target_word     : taulupe\n",
      "context_indices : [11404  5338  1542  4282  4283    -1    -1    -1    -1    -1]\n",
      "context_words   : ['faletau']\n",
      "context_words   : ['believes']\n",
      "context_words   : ['winning']\n",
      "context_words   : ['rbs']\n",
      "context_words   : ['6']\n",
      "target  : 11403\n",
      "context : [11404  5338  1542  4282  4283    -1    -1    -1    -1    -1]\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "index = 0\n",
    "print(f\"target_index    : {test_targets[index]}\")\n",
    "print(f\"target_word     : {inverse_vocab[test_targets[index]]}\")\n",
    "print(f\"context_indices : {test_contexts[index]}\")\n",
    "for c in test_contexts[index]:\n",
    "    if c == -1:\n",
    "        continue\n",
    "    print(f\"context_words   : {[inverse_vocab[c]]}\")\n",
    "\n",
    "print(\"target  :\", test_targets[index])\n",
    "print(\"context :\", test_contexts[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a function that takes in as input target and vocab length\n",
    "# outputs a one-hot vector, x_hot of dimension = (vocab_length,)\n",
    "def get_x_hot(target_idx, vocab_length):\n",
    "    x_hot = np.zeros(vocab_length, dtype=float)\n",
    "    x_hot[target_idx] = 1.0\n",
    "    return jnp.array(x_hot)\n",
    "\n",
    "def get_x_hot(target_idx, vocab_length):\n",
    "    x_hot = jnp.zeros(vocab_length, dtype=jnp.float32)\n",
    "    x_hot = x_hot.at[target_idx].set(1.0)\n",
    "    return x_hot\n",
    "\n",
    "get_x_hot(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 1., 1., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_y_true(context_idxs, vocab_length):\n",
    "    y_true = np.zeros(vocab_length)\n",
    "    for i in context_idxs:\n",
    "        y_true[int(i)] = 1.\n",
    "    return jnp.array(y_true)\n",
    "get_y_true(np.array([1, 2, 3]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.09003057, 0.24472848, 0.66524094], dtype=float32)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=0)\n",
    "\n",
    "x = softmax(np.array([1.,2.,3.]))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a forward pass in the skip gram model \n",
    "def net(target_hot, V, U):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    x_hot is one-hot vector, dimensions = (|v| x 1)\n",
    "    V: input embedding matrix, dimension = (n x |v|)\n",
    "    U: output embedding matrix, dimension = (|v| x n)\n",
    "        |v| = vocab size\n",
    "        n = no. of embedding dimensions\n",
    "    Output\n",
    "    z: score vector, dimension = (|v| x 1)\n",
    "    \"\"\"\n",
    "    return jnp.array(U @ V @ target_hot)\n",
    "\n",
    "n = 300\n",
    "v = 10000\n",
    "V = np.random.normal(0, 1, size=(n, v))\n",
    "U = np.random.normal(0, 1, size=(v, n))\n",
    "x_hot = get_x_hot(3, v)\n",
    "z = net(x_hot, V, U)\n",
    "type(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define local loss func\n",
    "def local_loss(params, target, context):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    params = [V, U]\n",
    "    V = input embedding matrix, dimensions = (n x |v|)\n",
    "    U = output embedding matrix, dimensions = (|v| x n)\n",
    "    target: target index, e,g. (98,)\n",
    "    context: context indices, e.g. (94, 95, 95, 97, 99, 100, 101, None, None), dimensions = (2*window_size)\n",
    "    \n",
    "    Output\n",
    "    loss_value: real number\n",
    "    \"\"\"\n",
    "    V = params[0] #; print(f'V shape: {V.shape} \\t type: {type(V)}')\n",
    "    U = params[1] #; print(f'U shape: {U.shape} \\t type: {type(U)}')\n",
    "    x_hot = get_x_hot(target, len(U)) # ; print(f'x_hot shape: {x_hot.shape} \\t type: {type(x_hot)}')\n",
    "    # y_true = get_y_true(context)\n",
    "    z = net(x_hot, V, U) #; print(f'z shape: {z.shape} \\t type: {type(z)}')\n",
    "    y_hat = softmax(z) #; print(f'y_hat shape: {y_hat.shape} \\t type: {type(y_hat)}')\n",
    "    loss = 0\n",
    "    for c in context:\n",
    "        if jnp.bool_(c == -1):\n",
    "            continue\n",
    "        loss += y_hat[c]\n",
    "    return -1*loss\n",
    "\n",
    "# n = 300; v = 10000\n",
    "# V = np.random.normal(0, 1, size=(n, v)) / np.sqrt(v)\n",
    "# U = np.random.normal(0, 1, size=(v, n)) / np.sqrt(v)\n",
    "# p = [V, U]\n",
    "# t = targets[0]\n",
    "# c = contexts[0]\n",
    "# local_loss(p, t, c)\n",
    "# loss_value_and_grad = jax.value_and_grad(local_loss, argnums=0)\n",
    "# v, g = loss_value_and_grad(p, t, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_all = jax.vmap(local_loss, in_axes=(None, 0, 0))\n",
    "\n",
    "@jax.jit\n",
    "def loss(params, targets, contexts):\n",
    "    \"\"\"return average of all the local losses\"\"\"\n",
    "    all_losses = loss_all(params, targets, contexts)\n",
    "    print(f'all_losses shape: {all_losses.shape}')\n",
    "    return jnp.mean(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: traced array with shape bool[].\nThe problem arose with the `bool` function. \nThis BatchTracer with object id 140548154691152 was created on line:\n  /var/folders/_3/8z9s_23x6w349w1_9vlqlhzh0000gn/T/ipykernel_1631/3823550736.py:22 (local_loss)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m cs \u001b[38;5;241m=\u001b[39m contexts[:\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m      8\u001b[0m loss_value_and_grad \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit( jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(loss) )\n\u001b[0;32m----> 9\u001b[0m v, g  \u001b[38;5;241m=\u001b[39m \u001b[43mloss_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 32 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[193], line 6\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(params, targets, contexts)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(params, targets, contexts):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"return average of all the local losses\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     all_losses \u001b[38;5;241m=\u001b[39m \u001b[43mloss_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_losses shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_losses\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean(all_losses)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[192], line 22\u001b[0m, in \u001b[0;36mlocal_loss\u001b[0;34m(params, target, context)\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m context:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mbool_(c \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y_hat[c]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Y4S1/DSA4212/Word-Embeddings/.venv/lib/python3.8/site-packages/jax/_src/core.py:1370\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m-> 1370\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: traced array with shape bool[].\nThe problem arose with the `bool` function. \nThis BatchTracer with object id 140548154691152 was created on line:\n  /var/folders/_3/8z9s_23x6w349w1_9vlqlhzh0000gn/T/ipykernel_1631/3823550736.py:22 (local_loss)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "# get the loss value and gradient\n",
    "n = 300; v = 10000\n",
    "V = np.random.normal(0, 1, size=(n, v)) / np.sqrt(v)\n",
    "U = np.random.normal(0, 1, size=(v, n)) / np.sqrt(v)\n",
    "p = [V, U]\n",
    "ts = targets[:5]\n",
    "cs = contexts[:5]\n",
    "loss_value_and_grad = jax.jit( jax.value_and_grad(loss) )\n",
    "v, g  = loss_value_and_grad(p, ts, cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 1.,  2., nan], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array([1., 2., None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
