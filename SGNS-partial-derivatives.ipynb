{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (13368, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A middle-school teacher in China has inked hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man convicted of killing the father and sist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avid rugby fan Prince Harry could barely watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Triple M Radio producer has been inundated w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Sally Forrest, an actress-dancer who graced th...\n",
       "1  A middle-school teacher in China has inked hun...\n",
       "2  A man convicted of killing the father and sist...\n",
       "3  Avid rugby fan Prince Harry could barely watch...\n",
       "4  A Triple M Radio producer has been inundated w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/raw data/raw_data.csv', header=0, names=['text'], usecols=[1])\n",
    "print(f'Data Shape: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "punctuations = string.punctuation\n",
    "def remove_punctuation(txt):\n",
    "    for char in punctuations:\n",
    "        if char in txt:\n",
    "            txt = txt.replace(char, \"\")\n",
    "    return txt\n",
    "\n",
    "# change to lower caps\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# remove punctuations\n",
    "data['text'] = data['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# read stopwords from data/raw data/stopwords.txt\n",
    "stop_words = []\n",
    "with open('./data/raw data/stopwords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip())\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    txt = [word for word in txt.split() if word not in stop_words]\n",
    "    return ' '.join(txt)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows of data: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [cnnafter, handful, events, months, hillary, c...\n",
       "1    [account, manager, quit, job, professional, vl...\n",
       "2    [missing, philae, space, probe, bumped, onto, ...\n",
       "3    [cnnigniting, live, cage, severing, heads, doz...\n",
       "4    [final, scoreline, suggested, otherwise, gordo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each row into list of words\n",
    "data_lst = data['text'].apply(lambda txt: txt.split(\" \"))\n",
    "\n",
    "# select number of rows to be used as training data\n",
    "nrows = 200\n",
    "random_indices = np.random.randint(low=0, high=len(data_lst), size=nrows)\n",
    "data_lst = data_lst[random_indices].reset_index(drop=True)\n",
    "\n",
    "print(f'Number of rows of data: {len(data_lst)}')\n",
    "data_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14519\n"
     ]
    }
   ],
   "source": [
    "# vocab dict\n",
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "for line in data_lst:\n",
    "    for word in line:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse_vocab dict\n",
    "inverse_vocab = {}\n",
    "for word, index in vocab.items():\n",
    "    inverse_vocab[index] = word\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "sequences = []\n",
    "for line in data_lst:\n",
    "    vectorized_line = [vocab[word] for word in line]\n",
    "    sequences.append(vectorized_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# choose 20 random sequences\n",
    "ntest = 20\n",
    "test_indices = np.random.randint(low=0, high=len(sequences), size=ntest)\n",
    "test_sequences = [sequences[i] for i in test_indices]\n",
    "train_sequences = [sequences[i] for i in range(len(sequences)) if i not in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate samples\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0,\n",
    "          shuffle=True)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.reshape(tf.constant([context_word], dtype=\"int64\"), (1,1))\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate testing data\n",
    "def generate_testing_data(sequences, vocab_size, window_size):\n",
    "    targets, contexts, labels = [], [], []\n",
    "    for sequence in tqdm(sequences):\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=vocab_size,\n",
    "            window_size=window_size,\n",
    "            negative_samples=0)\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "        targets.append(target_word)\n",
    "        contexts.append(context_word)\n",
    "        labels.append(1)\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [02:29<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape: (309499,)\n",
      "contexts shape: (309499, 5)\n",
      "labels shape: (309499, 5)\n"
     ]
    }
   ],
   "source": [
    "# generate training data\n",
    "window_size = 5\n",
    "num_ns = 4\n",
    "vocab_size = len(vocab)\n",
    "seed = 4212\n",
    "\n",
    "targets, contexts, labels = generate_training_data(sequences=train_sequences,\n",
    "                                                 window_size=window_size,\n",
    "                                                 num_ns=num_ns,\n",
    "                                                 vocab_size=vocab_size,\n",
    "                                                 seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f'targets shape: {targets.shape}')\n",
    "print(f'contexts shape: {contexts.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 158.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets_test shape: (3970,)\n",
      "contexts_test shape: (3970,)\n",
      "labels_test shape: (3970,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate testing data\n",
    "targets_test, contexts_test, labels_test = generate_testing_data(sequences=test_sequences,\n",
    "                                                                    vocab_size=vocab_size,\n",
    "                                                                    window_size=window_size)\n",
    "\n",
    "targets_test = np.array(targets_test)\n",
    "contexts_test = np.array(contexts_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "print(f'targets_test shape: {targets_test.shape}')\n",
    "print(f'contexts_test shape: {contexts_test.shape}')\n",
    "print(f'labels_test shape: {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check on quality of training and testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 120\n",
      "target_word     : irish\n",
      "context_indices : [ 128   38 1557 1567 1050]\n",
      "context_words   : ['welsh', 'gala', 'portuguese', 'prodded', 'wee']\n",
      "label           : [1 0 0 0 0]\n",
      "target  : 120\n",
      "context : [ 128   38 1557 1567 1050]\n",
      "label   : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "print(f\"target_index    : {targets[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets[0]]}\")\n",
    "print(f\"context_indices : {contexts[0]}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c] for c in contexts[0]]}\")\n",
    "print(f\"label           : {labels[0]}\")\n",
    "\n",
    "print(\"target  :\", targets[0])\n",
    "print(\"context :\", contexts[0])\n",
    "print(\"label   :\", labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 709\n",
      "target_word     : health\n",
      "context_index : 14466\n",
      "context_word   : cadwaladr\n",
      "label           : 1\n",
      "target  : 709\n",
      "context : 14466\n",
      "label   : 1\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "print(f\"target_index    : {targets_test[0]}\")\n",
    "print(f\"target_word     : {inverse_vocab[targets_test[0]]}\")\n",
    "print(f\"context_index : {contexts_test[0]}\")\n",
    "print(f\"context_word   : {inverse_vocab[contexts_test[0]]}\")\n",
    "print(f\"label           : {labels_test[0]}\")\n",
    "\n",
    "print(\"target  :\", targets_test[0])\n",
    "print(\"context :\", contexts_test[0])\n",
    "print(\"label   :\", labels_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Objective Function for SGNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\min_{\\theta} = \\frac{1}{N} \\sum_{i=1}^{N} [log \\sigma(u_{ic}^T)  + \\sum_{k=1}^{K}log \\sigma(-u_{kc}^T v_{iw})]\n",
    "\n",
    "\\\\\n",
    "\\\\\n",
    "\\theta = [U, V]\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"Inputs a real number, outputs a real number\"\"\"\n",
    "    return 1 / (1 + jnp.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get embedding vectors\n",
    "def get_embedding_vectors(target, context, V, U):\n",
    "    \"\"\"\n",
    "    Input (example)\n",
    "    target = (188,)\n",
    "    context = (93, 40, 1648, 1659, 1109)\n",
    "    V: matrix of dim (n x |v|)\n",
    "    U: matrix of dim (|v| x n)\n",
    "        n = embedding dimension, |v| = vocab size\n",
    "\n",
    "    Output\n",
    "    v_t: target word vector, dimension: (n,)\n",
    "    u_c: context word vectors, consists of u_pos and u_neg: dimension: (n, len(context))\n",
    "    \"\"\"\n",
    "    target = target.astype(int)\n",
    "    context = context.astype(int)\n",
    "    v_t = V.T[target]\n",
    "    u_c = U[context]\n",
    "    return v_t, u_c\n",
    "\n",
    "# t = targets[0]\n",
    "# c = contexts[0]\n",
    "# v_test, u_test = get_embedding_vectors(t, c, V, U)\n",
    "# print(f'v_test shape: {v_test.shape}')\n",
    "# print(f'u_test shape: {u_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define local_loss function\n",
    "@jax.jit\n",
    "def local_loss(params):\n",
    "    \"\"\"\n",
    "    Input (example)\n",
    "    params = [v_t, jnp.array([u_pos, u_neg])]\n",
    "        v_t: target word vector, dimension: (n,)\n",
    "        u_c: context word vectors, consists of u_pos and u_neg: dimension: (len(context), n)\n",
    "\n",
    "    Output\n",
    "    local_loss: real number\n",
    "    \"\"\"\n",
    "    v_t = params[0]\n",
    "    u_c = params[1]\n",
    "    return -jnp.log(sigmoid(jnp.dot(u_c[0], v_t))) - jnp.sum(jnp.log(sigmoid(-jnp.dot(u_c[1:], v_t))))\n",
    "\n",
    "# p = [V[:, 0], U[:5, :]]\n",
    "# print(f'local_loss: {local_loss(p)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define gradient function\n",
    "L_grad = jax.grad(local_loss)\n",
    "\n",
    "# g = L_grad([V[:, 0], U[:5, :]])\n",
    "# print(f\"g[0] shape: {g[0].shape}\")\n",
    "# print(f\"g[1] shape: {g[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to update parameters\n",
    "# def update_params(target, context, partial_v, partial_u, V, U):\n",
    "#     t = target; print(f'target: {type(t)}')\n",
    "#     c = context; print(f'context: {type(c)}')\n",
    "#     V[:, t] = V[:, t] - lr * partial_v\n",
    "#     U[c, :] = U[c, :] - lr * partial_u\n",
    "\n",
    "def update_params(target, context, partial_v, partial_u, V, U):\n",
    "    # Update V and U\n",
    "    V_updated = V.at[:, target].add(-lr * partial_v)\n",
    "    U_updated = U.at[context, :].add(-lr * partial_u)\n",
    "\n",
    "    return V_updated, U_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch functions\n",
    "get_batch_embedding_vectors = jax.vmap(get_embedding_vectors, in_axes=(0, 0, None, None))\n",
    "\n",
    "batch_losses = jax.vmap(local_loss, in_axes=(0))\n",
    "\n",
    "batch_grads = jax.vmap(L_grad, in_axes=(0))\n",
    "\n",
    "batch_update_params = jax.vmap(update_params, in_axes=(0, 0, 0, 0, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V shape: (300, 14519)\n",
      "U shape: (14519, 300)\n",
      "targets_data shape: (309499,) \t type: ArrayImpl\n",
      "contexts_data shape: (309499, 5) \t type: ArrayImpl\n",
      "labels_data shape: (309499, 5) \t type: ArrayImpl\n"
     ]
    }
   ],
   "source": [
    "# set up\n",
    "n = 300\n",
    "v = len(vocab)\n",
    "V = jnp.array(np.random.normal(0, 1, size=(n, v)) / np.sqrt(v))\n",
    "U = jnp.array(np.random.normal(0, 1, size=(v, n)) / np.sqrt(v))\n",
    "\n",
    "params = [V, U]\n",
    "targets_data = jnp.array(targets.astype(float))\n",
    "contexts_data = jnp.array(contexts.astype(float))\n",
    "labels_data = jnp.array(labels.astype(float))\n",
    "\n",
    "print(f'V shape: {V.shape}')\n",
    "print(f'U shape: {U.shape}')\n",
    "print(f'targets_data shape: {targets_data.shape} \\t type: {type(targets_data).__name__}')\n",
    "print(f'contexts_data shape: {contexts_data.shape} \\t type: {type(contexts_data).__name__}')\n",
    "print(f'labels_data shape: {labels_data.shape} \\t type: {type(labels_data).__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/309 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/309 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Indexer must have integer or boolean type, got indexer with type float32 at position 1, indexer value Traced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([ 6837.,  9358.,  8486., 13757.,  6831.,  3024.,   976.,   648.,\n       10064.,  9505.,   296.,  7598., 11652.,  3101.,  9277., 12584.,\n       11340.,  1044.,  8537.,  1257., 10175.,  6819.,  6354.,  6369.,\n        3074.,   684.,  5253.,  4097., 13407.,  1133., 12121.,  9409.,\n       14395.,  3654.,  4420.,  8502.,   846.,  7232.,  2572., 11862.,\n        1541.,  6562., 14391.,  9669., 10520.,  2263.,  1339.,  7096.,\n        1834.,  4628.,  6641.,  5897.,  1284., 11829.,  6472.,  8953.,\n        4005.,  4673.,  7956.,  2556.,  5122.,  3722.,  1313., 11397.,\n        4021.,  2469., 13226.,  1468.,   888.,  1902.,  5171.,  1689.,\n        6466.,  8192.,  3573.,  2815.,  5617., 10437.,  3110.,  7802.,\n       11910.,  2488.,  8364., 11290.,   902., 14353.,  9152.,  1481.,\n        4397.,   316.,  7635.,  7067.,  1389.,  5652.,  8946.,   497.,\n        9814.,  7318.,  8535.,  3559.,  1159.,  1753.,  7154.,  4229.,\n        6099., 12700.,  3093.,  2728.,  2203., 12566.,  3902.,  5963.,\n       13437.,  3727.,   694.,  5795.,  2395.,  2285.,  6954.,  1420.,\n        2131.,  8668., 13153.,  3513.,  5061.,  7757.,  3281.,  4969.,\n       10643., 11136.,  1490.,  1308.,   747.,  3924.,  1050.,  3403.,\n       10127., 10369.,  3798.,  6757.,  4713.,  2622., 12094., 13047.,\n        5855.,  4643.,  8381.,   747.,  9661.,  7481.,  7799.,  2815.,\n        3581.,   487., 10853., 11469.,  5176.,  9779.,  9120.,  7296.,\n        8044.,  6663.,  4880.,  2740.,  3619.,  6597.,  3814.,  3770.,\n        3524., 13789.,  4468.,  1979., 11546.,  4734.,  8183.,  3564.,\n         875.,   112.,  1990.,  2219., 13638.,  1140., 11823.,  5751.,\n         839.,  2686., 12791.,  7115., 10779.,  2917., 13825.,  9080.,\n       11382.,  9027.,  4425.,   475.,   227., 10979.,  8520.,   875.,\n       11828.,  3040.,  8170.,  3186.,  6471.,  2734.,  6883.,  4290.,\n         980., 13346., 10108.,  1363.,  5870., 10761.,  7065.,  9807.,\n       11012.,  1268., 14273.,  4513.,  9494.,  6508.,  1111., 10595.,\n       13850.,  4155.,  3639., 13574., 12770., 12839.,  2297.,  4114.,\n       13158.,  2264., 13548.,  8035., 10437.,  7629., 10156., 10445.,\n        2621.,  1395.,  4307.,    63.,  1158.,   320.,  3391.,   413.,\n        9754.,  5812.,   964.,  2683.,  9357.,  1056., 13790.,  1317.,\n        5956.,  1328.,  2762.,  3557.,  9358.,  5161.,  3177., 13540.,\n        7623.,  1933., 11315., 13834., 10076.,  3104.,  1223.,  9700.,\n        6243.,  4657.,  5229.,  8590.,  5786.,  4416.,   616., 11219.,\n       10300.,  4772.,  9236.,  3349.,  3285.,  2602.,  5112., 12790.,\n        6082., 11992., 14401., 12591.,  3309.,  9194.,  8821.,  6025.,\n        9835.,   517.,  7760., 10109., 10068.,  4420.,  5329., 13333.,\n        8603.,    76.,  3398.,  5645.,  3980.,  4148., 14175.,   623.,\n        5225.,  4238.,  5248.,  2937., 10826.,  3466.,  5627.,   996.,\n         459.,  4021.,  3304., 14011.,  8961.,  5764.,  8513.,  2381.,\n        1306.,  5163., 14197., 12969.,  4757.,  9463., 10510.,  6186.,\n        8910.,  7928., 10948.,  1937.,  9265.,  1990.,  7217.,  8572.,\n        1893.,  1422.,  9398., 11439.,  9630.,  1327.,  3223.,  4716.,\n        2864.,  1540.,  3661., 11082.,  7277.,  8908.,  1499.,  4021.,\n       13158.,  2283.,  7113.,  3751.,   779.,  5491.,  8104.,  2379.,\n       13785., 13851.,  5526.,  5416.,  6523.,  3692.,  7634.,  3987.,\n        2902.,  1911.,  5202.,  9215.,  4422.,  4161., 11015., 10983.,\n        7068.,  3401., 13431., 12935.,  3108., 14223.,  3702., 11170.,\n        1484.,   577.,  2998., 13239.,  1877.,  3009.,  9560.,  4322.,\n        9126.,  2637., 11065.,  1647.,  5497.,   847.,  9324.,  7949.,\n        1529.,  5957.,  3128.,  4647.,  3661.,  3740.,   515.,  6926.,\n        2972.,  4772.,  4905.,  4896.,  8503.,  6959.,  9047.,  2444.,\n        7367.,  8846.,  1607., 14028.,  1924., 13403.,  7113.,  7251.,\n        4510.,   316.,  7065.,  1476., 10150.,  9670.,  8508.,  5448.,\n        5073.,  2445., 13592.,  6279.,  6357.,  6879.,  3872., 10855.,\n        1956.,  4801., 13049., 14261.,  3135.,  7966.,  1577.,  2973.,\n        2214.,  6900.,  5985.,   511., 13630.,  3692.,  1396.,  6763.,\n       11219.,  2263.,   545.,  6153., 10580., 11307.,  1956.,  3171.,\n        3550.,  1773.,   845.,  8609., 13437.,  1193.,  7967.,  1245.,\n       12861.,  2533.,  4468.,  1007.,  3432.,  5694.,  1777.,  1397.,\n        7030., 10638.,  3987.,  6388.,  3090.,  8502.,  3344.,  7106.,\n        5485.,   218.,  1012.,   748.,  8736.,  6957.,  5793.,  7154.,\n       12502.,  2995.,  8958., 11456., 10149.,  3735.,  1275.,  4778.,\n        2516.,  4518., 10580.,  4819.,  3098., 12558.,  4071.,  7975.,\n        8332.,  8861.,  8143.,  6829.,  4602.,  1541.,  9633.,  8525.,\n        2363.,  4241., 11861.,  1879.,  7715.,  3264., 13231.,  3713.,\n        7169.,  4822.,  6016.,  2911.,  2210.,  3573., 11456.,  1138.,\n       11127., 10478.,  3321.,  4021.,  4696.,   547., 12258.,  7469.,\n        3294., 10608., 12027.,  2525.,  1850.,  5055., 11146.,  8895.,\n       10978.,  6341.,  7980.,  1105.,  8219.,  6457.,  6977.,  7996.,\n        8537.,  6923.,  8910.,  8599., 14123.,  2565., 11751.,  3236.,\n        1228.,  9600.,  5393.,  3345.,  8428., 13677.,  9521.,  1311.,\n        5097.,  6925., 11215.,  5103.,  3395.,  9389.,  4672.,  6887.,\n       13044.,   152.,  7577.,  2800., 13786.,  6230.,  9180., 12816.,\n        3523., 12063.,  1313., 10856.,  1592.,  9970.,  4674.,  8005.,\n        3791., 12065., 10437., 12245.,   413.,  1223.,  2063., 14418.,\n         262., 12941.,   680.,  7796.,  7487.,  1754.,  6056.,  2356.,\n        3372.,  2813.,  6087.,  5002.,  4638.,  6340.,  2656., 10722.,\n       11217.,  8402.,  7744.,  3640.,   744.,  5285.,  6165.,  2917.,\n        1710.,  9968., 13306.,  2718.,  8482.,  2016.,  6082., 11560.,\n       10239.,  5454.,  2753.,  1499., 11861.,  6073.,  1661., 13427.,\n         364.,  3066.,  4566.,  8506.,  8124., 10160.,  9220.,  1928.,\n        3457.,   638., 13858., 10766.,  3489.,  5354.,  9609.,  2196.,\n        8364.,  2034.,  6267., 11397., 10360.,  5983., 13827.,  8656.,\n        4473.,  3672.,  6115.,  6716.,  5361.,  2189.,  1308.,  5944.,\n        6207.,  7468.,  3694.,  2085.,  2853.,  8462.,  6194.,  4249.,\n        3313.,  8377.,  2550.,  4937.,  2283.,  9831., 11788.,  4033.,\n        6047.,  6395.,  6153., 12591.,  1774.,   647.,  1020.,  1765.,\n        8222.,   284.,  2248., 12365.,  6114.,  6288.,  9663., 12962.,\n        8368., 12922.,  1715.,  8848.,  4404., 14103.,  1389.,  2346.,\n        8353.,  4397.,  6111.,  5254.,  4607.,  1360., 14255.,  4693.,\n        7518., 10891., 11652.,  6509.,  5468., 13067., 13202.,  4783.,\n         744., 11534., 14292.,  5890.,  1994.,  5696., 10370.,  3015.,\n        4887.,  7834.,  2861.,  7744.,   723., 12746., 13586.,  5088.,\n         181.,  3384., 14143.,  4042., 11641.,  5480., 12257.,  5419.,\n       11104.,  9916.,   431.,  4995.,  3736.,  1284.,  6817.,  9840.,\n        8957.,  8419.,  2508., 13277.,  3810., 11795., 10865.,  1600.,\n       10881.,  2343.,  4098.,  1728.,  8848.,  9106., 14002.,  5136.,\n        7368.,  2853.,  4452.,  6065., 10718.,  5353., 11512., 10853.,\n       13084.,  6024., 13238.,  5136.,  7209.,  8159.,  2587.,   763.,\n       10203.,  3733.,  9320., 12771., 12120.,   891.,  2759., 10425.,\n        3393.,  9082.,  8953., 11651.,  7208.,  1844.,  3775.,  8165.,\n        4307., 11554.,  2742.,  7636.,  3516.,  3907.,  9242.,  7206.,\n       13380.,  5657., 10880.,  6586., 11871.,  2808., 10027.,  3221.,\n        1813., 10623., 10194.,  6046.,   842.,  3872.,  9496.,  9209.,\n        2264., 12970.,  2301.,  5783.,  7798., 10217.,  3987.,  3787.,\n        8584.,  2777.,  2219.,  4475.,  2399.,  6161.,  4476.,  3917.,\n         687.,  8613.,  8435.,  6845.,  6124.,  1376.,  3565.,  9264.,\n        6608., 13998.,  5080.,  3569.,  9628.,  4700.,  3530.,  1255.,\n        9940.,   691.,   443., 14086.,  6658.,  6649.,  5114.,  5053.,\n        1313.,  4365., 13028.,  3504., 11300.,   449., 11011.,  1392.,\n       11475.,  5378.,  2053.,  5939.,  2411.,   145.,  3692.,  5968.,\n        3496.,  7809.,  3506.,  1702.,  5999.,  6353.,  3401.,  3802.,\n        9559.,  3843.,  1676.,  3823.,  2266.,  9034.,  2968.,  1805.,\n        3466.,  9349.,  1300.,  9572.,  8915.,  3434.,  4807.,  5583.,\n        3545.,  8079., 11536., 11841., 12337.,   693.,  3235., 10591.,\n        3793.,  6051., 13839., 12947.,  3791., 11552.,  5972., 12622.,\n       11550.,  1896.,  2562.,  7711.,  1520.,  2947., 11754.,  7920.,\n        3877.,  4028.,  7476., 12321., 14374., 10246., 11418.,   680.,\n        4905., 10278., 14354.,  7152.,  3174.,  1655., 11161.,   218.,\n        8249., 10923., 12375.,  3584.,   218., 11417., 11606.,   535.,\n        3431.,  3135., 12113., 13408.,  1798.,  1403.,  3401.,  9905.,\n        3561.,  2218., 11699.,  8148.,   931.,  3950.,  3382., 13343.,\n       10003.,  1921., 14192.,  6902.,  5739., 10797.,   780.,  2675.],      dtype=float32)\n  batch_dim = 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m partial_V_batch \u001b[38;5;241m=\u001b[39m grads[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     42\u001b[0m partial_U_batch \u001b[38;5;241m=\u001b[39m grads[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m \u001b[43mbatch_update_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_V_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_U_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# store the loss value\u001b[39;00m\n\u001b[1;32m     47\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(loss_values))\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[65], line 10\u001b[0m, in \u001b[0;36mupdate_params\u001b[0;34m(target, context, partial_v, partial_u, V, U)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_params\u001b[39m(target, context, partial_v, partial_u, V, U):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Update V and U\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     V_updated \u001b[38;5;241m=\u001b[39m \u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpartial_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     U_updated \u001b[38;5;241m=\u001b[39m U\u001b[38;5;241m.\u001b[39mat[context, :]\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;241m-\u001b[39mlr \u001b[38;5;241m*\u001b[39m partial_u)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m V_updated, U_updated\n",
      "File \u001b[0;32m~/Documents/Y4S1/DSA4212/Word-Embeddings/.venv/lib/python3.8/site-packages/jax/_src/numpy/array_methods.py:485\u001b[0m, in \u001b[0;36mallow_pass_by_position_with_warning.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs[:n_positional], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverted_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Y4S1/DSA4212/Word-Embeddings/.venv/lib/python3.8/site-packages/jax/_src/numpy/array_methods.py:570\u001b[0m, in \u001b[0;36m_IndexUpdateRef.add\u001b[0;34m(self, values, indices_are_sorted, unique_indices, mode)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;129m@allow_pass_by_position_with_warning\u001b[39m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\u001b[38;5;28mself\u001b[39m, values, \u001b[38;5;241m*\u001b[39m, indices_are_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, unique_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    562\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    563\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Pure equivalent of ``x[idx] += y``.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m  Returns the value of ``x`` that would result from the NumPy-style\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m  See :mod:`jax.ops` for details.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scatter_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mindices_are_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices_are_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43munique_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munique_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Y4S1/DSA4212/Word-Embeddings/.venv/lib/python3.8/site-packages/jax/_src/ops/scatter.py:76\u001b[0m, in \u001b[0;36m_scatter_update\u001b[0;34m(x, idx, y, scatter_op, indices_are_sorted, unique_indices, mode, normalize_indices)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# XLA gathers and scatters are very similar in structure; the scatter logic\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# is more or less a transpose of the gather equivalent.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m treedef, static_idx, dynamic_idx \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39m_split_index_for_jit(idx, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_scatter_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscatter_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreedef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mindices_are_sorted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mnormalize_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Y4S1/DSA4212/Word-Embeddings/.venv/lib/python3.8/site-packages/jax/_src/ops/scatter.py:98\u001b[0m, in \u001b[0;36m_scatter_impl\u001b[0;34m(x, y, scatter_op, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, normalize_indices)\u001b[0m\n\u001b[1;32m     92\u001b[0m   warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscatter inputs have incompatible types: cannot safely cast \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue from dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlax\u001b[38;5;241m.\u001b[39mdtype(y)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlax\u001b[38;5;241m.\u001b[39mdtype(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn future JAX releases this will result in an error.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     95\u001b[0m                 \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m     97\u001b[0m idx \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39m_merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n\u001b[0;32m---> 98\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_to_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnormalize_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Avoid calling scatter if the slice shape is empty, both as a fast path and\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# to handle cases like zeros(0)[array([], int32)].\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m core\u001b[38;5;241m.\u001b[39mis_empty_shape(indexer\u001b[38;5;241m.\u001b[39mslice_shape):\n",
      "File \u001b[0;32m~/Documents/Y4S1/DSA4212/Word-Embeddings/.venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:4440\u001b[0m, in \u001b[0;36m_index_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m   4436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (abstract_i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   4437\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m (issubdtype(abstract_i\u001b[38;5;241m.\u001b[39mdtype, integer) \u001b[38;5;129;01mor\u001b[39;00m issubdtype(abstract_i\u001b[38;5;241m.\u001b[39mdtype, bool_))):\n\u001b[1;32m   4438\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexer must have integer or boolean type, got indexer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4439\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m at position \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, indexer value \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4440\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(abstract_i\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname, idx_pos, i))\n\u001b[1;32m   4442\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing mode not yet supported. Open a feature request!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4443\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(idx))\n",
      "\u001b[0;31mTypeError\u001b[0m: Indexer must have integer or boolean type, got indexer with type float32 at position 1, indexer value Traced<ShapedArray(float32[])>with<BatchTrace(level=1/0)> with\n  val = Array([ 6837.,  9358.,  8486., 13757.,  6831.,  3024.,   976.,   648.,\n       10064.,  9505.,   296.,  7598., 11652.,  3101.,  9277., 12584.,\n       11340.,  1044.,  8537.,  1257., 10175.,  6819.,  6354.,  6369.,\n        3074.,   684.,  5253.,  4097., 13407.,  1133., 12121.,  9409.,\n       14395.,  3654.,  4420.,  8502.,   846.,  7232.,  2572., 11862.,\n        1541.,  6562., 14391.,  9669., 10520.,  2263.,  1339.,  7096.,\n        1834.,  4628.,  6641.,  5897.,  1284., 11829.,  6472.,  8953.,\n        4005.,  4673.,  7956.,  2556.,  5122.,  3722.,  1313., 11397.,\n        4021.,  2469., 13226.,  1468.,   888.,  1902.,  5171.,  1689.,\n        6466.,  8192.,  3573.,  2815.,  5617., 10437.,  3110.,  7802.,\n       11910.,  2488.,  8364., 11290.,   902., 14353.,  9152.,  1481.,\n        4397.,   316.,  7635.,  7067.,  1389.,  5652.,  8946.,   497.,\n        9814.,  7318.,  8535.,  3559.,  1159.,  1753.,  7154.,  4229.,\n        6099., 12700.,  3093.,  2728.,  2203., 12566.,  3902.,  5963.,\n       13437.,  3727.,   694.,  5795.,  2395.,  2285.,  6954.,  1420.,\n        2131.,  8668., 13153.,  3513.,  5061.,  7757.,  3281.,  4969.,\n       10643., 11136.,  1490.,  1308.,   747.,  3924.,  1050.,  3403.,\n       10127., 10369.,  3798.,  6757.,  4713.,  2622., 12094., 13047.,\n        5855.,  4643.,  8381.,   747.,  9661.,  7481.,  7799.,  2815.,\n        3581.,   487., 10853., 11469.,  5176.,  9779.,  9120.,  7296.,\n        8044.,  6663.,  4880.,  2740.,  3619.,  6597.,  3814.,  3770.,\n        3524., 13789.,  4468.,  1979., 11546.,  4734.,  8183.,  3564.,\n         875.,   112.,  1990.,  2219., 13638.,  1140., 11823.,  5751.,\n         839.,  2686., 12791.,  7115., 10779.,  2917., 13825.,  9080.,\n       11382.,  9027.,  4425.,   475.,   227., 10979.,  8520.,   875.,\n       11828.,  3040.,  8170.,  3186.,  6471.,  2734.,  6883.,  4290.,\n         980., 13346., 10108.,  1363.,  5870., 10761.,  7065.,  9807.,\n       11012.,  1268., 14273.,  4513.,  9494.,  6508.,  1111., 10595.,\n       13850.,  4155.,  3639., 13574., 12770., 12839.,  2297.,  4114.,\n       13158.,  2264., 13548.,  8035., 10437.,  7629., 10156., 10445.,\n        2621.,  1395.,  4307.,    63.,  1158.,   320.,  3391.,   413.,\n        9754.,  5812.,   964.,  2683.,  9357.,  1056., 13790.,  1317.,\n        5956.,  1328.,  2762.,  3557.,  9358.,  5161.,  3177., 13540.,\n        7623.,  1933., 11315., 13834., 10076.,  3104.,  1223.,  9700.,\n        6243.,  4657.,  5229.,  8590.,  5786.,  4416.,   616., 11219.,\n       10300.,  4772.,  9236.,  3349.,  3285.,  2602.,  5112., 12790.,\n        6082., 11992., 14401., 12591.,  3309.,  9194.,  8821.,  6025.,\n        9835.,   517.,  7760., 10109., 10068.,  4420.,  5329., 13333.,\n        8603.,    76.,  3398.,  5645.,  3980.,  4148., 14175.,   623.,\n        5225.,  4238.,  5248.,  2937., 10826.,  3466.,  5627.,   996.,\n         459.,  4021.,  3304., 14011.,  8961.,  5764.,  8513.,  2381.,\n        1306.,  5163., 14197., 12969.,  4757.,  9463., 10510.,  6186.,\n        8910.,  7928., 10948.,  1937.,  9265.,  1990.,  7217.,  8572.,\n        1893.,  1422.,  9398., 11439.,  9630.,  1327.,  3223.,  4716.,\n        2864.,  1540.,  3661., 11082.,  7277.,  8908.,  1499.,  4021.,\n       13158.,  2283.,  7113.,  3751.,   779.,  5491.,  8104.,  2379.,\n       13785., 13851.,  5526.,  5416.,  6523.,  3692.,  7634.,  3987.,\n        2902.,  1911.,  5202.,  9215.,  4422.,  4161., 11015., 10983.,\n        7068.,  3401., 13431., 12935.,  3108., 14223.,  3702., 11170.,\n        1484.,   577.,  2998., 13239.,  1877.,  3009.,  9560.,  4322.,\n        9126.,  2637., 11065.,  1647.,  5497.,   847.,  9324.,  7949.,\n        1529.,  5957.,  3128.,  4647.,  3661.,  3740.,   515.,  6926.,\n        2972.,  4772.,  4905.,  4896.,  8503.,  6959.,  9047.,  2444.,\n        7367.,  8846.,  1607., 14028.,  1924., 13403.,  7113.,  7251.,\n        4510.,   316.,  7065.,  1476., 10150.,  9670.,  8508.,  5448.,\n        5073.,  2445., 13592.,  6279.,  6357.,  6879.,  3872., 10855.,\n        1956.,  4801., 13049., 14261.,  3135.,  7966.,  1577.,  2973.,\n        2214.,  6900.,  5985.,   511., 13630.,  3692.,  1396.,  6763.,\n       11219.,  2263.,   545.,  6153., 10580., 11307.,  1956.,  3171.,\n        3550.,  1773.,   845.,  8609., 13437.,  1193.,  7967.,  1245.,\n       12861.,  2533.,  4468.,  1007.,  3432.,  5694.,  1777.,  1397.,\n        7030., 10638.,  3987.,  6388.,  3090.,  8502.,  3344.,  7106.,\n        5485.,   218.,  1012.,   748.,  8736.,  6957.,  5793.,  7154.,\n       12502.,  2995.,  8958., 11456., 10149.,  3735.,  1275.,  4778.,\n        2516.,  4518., 10580.,  4819.,  3098., 12558.,  4071.,  7975.,\n        8332.,  8861.,  8143.,  6829.,  4602.,  1541.,  9633.,  8525.,\n        2363.,  4241., 11861.,  1879.,  7715.,  3264., 13231.,  3713.,\n        7169.,  4822.,  6016.,  2911.,  2210.,  3573., 11456.,  1138.,\n       11127., 10478.,  3321.,  4021.,  4696.,   547., 12258.,  7469.,\n        3294., 10608., 12027.,  2525.,  1850.,  5055., 11146.,  8895.,\n       10978.,  6341.,  7980.,  1105.,  8219.,  6457.,  6977.,  7996.,\n        8537.,  6923.,  8910.,  8599., 14123.,  2565., 11751.,  3236.,\n        1228.,  9600.,  5393.,  3345.,  8428., 13677.,  9521.,  1311.,\n        5097.,  6925., 11215.,  5103.,  3395.,  9389.,  4672.,  6887.,\n       13044.,   152.,  7577.,  2800., 13786.,  6230.,  9180., 12816.,\n        3523., 12063.,  1313., 10856.,  1592.,  9970.,  4674.,  8005.,\n        3791., 12065., 10437., 12245.,   413.,  1223.,  2063., 14418.,\n         262., 12941.,   680.,  7796.,  7487.,  1754.,  6056.,  2356.,\n        3372.,  2813.,  6087.,  5002.,  4638.,  6340.,  2656., 10722.,\n       11217.,  8402.,  7744.,  3640.,   744.,  5285.,  6165.,  2917.,\n        1710.,  9968., 13306.,  2718.,  8482.,  2016.,  6082., 11560.,\n       10239.,  5454.,  2753.,  1499., 11861.,  6073.,  1661., 13427.,\n         364.,  3066.,  4566.,  8506.,  8124., 10160.,  9220.,  1928.,\n        3457.,   638., 13858., 10766.,  3489.,  5354.,  9609.,  2196.,\n        8364.,  2034.,  6267., 11397., 10360.,  5983., 13827.,  8656.,\n        4473.,  3672.,  6115.,  6716.,  5361.,  2189.,  1308.,  5944.,\n        6207.,  7468.,  3694.,  2085.,  2853.,  8462.,  6194.,  4249.,\n        3313.,  8377.,  2550.,  4937.,  2283.,  9831., 11788.,  4033.,\n        6047.,  6395.,  6153., 12591.,  1774.,   647.,  1020.,  1765.,\n        8222.,   284.,  2248., 12365.,  6114.,  6288.,  9663., 12962.,\n        8368., 12922.,  1715.,  8848.,  4404., 14103.,  1389.,  2346.,\n        8353.,  4397.,  6111.,  5254.,  4607.,  1360., 14255.,  4693.,\n        7518., 10891., 11652.,  6509.,  5468., 13067., 13202.,  4783.,\n         744., 11534., 14292.,  5890.,  1994.,  5696., 10370.,  3015.,\n        4887.,  7834.,  2861.,  7744.,   723., 12746., 13586.,  5088.,\n         181.,  3384., 14143.,  4042., 11641.,  5480., 12257.,  5419.,\n       11104.,  9916.,   431.,  4995.,  3736.,  1284.,  6817.,  9840.,\n        8957.,  8419.,  2508., 13277.,  3810., 11795., 10865.,  1600.,\n       10881.,  2343.,  4098.,  1728.,  8848.,  9106., 14002.,  5136.,\n        7368.,  2853.,  4452.,  6065., 10718.,  5353., 11512., 10853.,\n       13084.,  6024., 13238.,  5136.,  7209.,  8159.,  2587.,   763.,\n       10203.,  3733.,  9320., 12771., 12120.,   891.,  2759., 10425.,\n        3393.,  9082.,  8953., 11651.,  7208.,  1844.,  3775.,  8165.,\n        4307., 11554.,  2742.,  7636.,  3516.,  3907.,  9242.,  7206.,\n       13380.,  5657., 10880.,  6586., 11871.,  2808., 10027.,  3221.,\n        1813., 10623., 10194.,  6046.,   842.,  3872.,  9496.,  9209.,\n        2264., 12970.,  2301.,  5783.,  7798., 10217.,  3987.,  3787.,\n        8584.,  2777.,  2219.,  4475.,  2399.,  6161.,  4476.,  3917.,\n         687.,  8613.,  8435.,  6845.,  6124.,  1376.,  3565.,  9264.,\n        6608., 13998.,  5080.,  3569.,  9628.,  4700.,  3530.,  1255.,\n        9940.,   691.,   443., 14086.,  6658.,  6649.,  5114.,  5053.,\n        1313.,  4365., 13028.,  3504., 11300.,   449., 11011.,  1392.,\n       11475.,  5378.,  2053.,  5939.,  2411.,   145.,  3692.,  5968.,\n        3496.,  7809.,  3506.,  1702.,  5999.,  6353.,  3401.,  3802.,\n        9559.,  3843.,  1676.,  3823.,  2266.,  9034.,  2968.,  1805.,\n        3466.,  9349.,  1300.,  9572.,  8915.,  3434.,  4807.,  5583.,\n        3545.,  8079., 11536., 11841., 12337.,   693.,  3235., 10591.,\n        3793.,  6051., 13839., 12947.,  3791., 11552.,  5972., 12622.,\n       11550.,  1896.,  2562.,  7711.,  1520.,  2947., 11754.,  7920.,\n        3877.,  4028.,  7476., 12321., 14374., 10246., 11418.,   680.,\n        4905., 10278., 14354.,  7152.,  3174.,  1655., 11161.,   218.,\n        8249., 10923., 12375.,  3584.,   218., 11417., 11606.,   535.,\n        3431.,  3135., 12113., 13408.,  1798.,  1403.,  3401.,  9905.,\n        3561.,  2218., 11699.,  8148.,   931.,  3950.,  3382., 13343.,\n       10003.,  1921., 14192.,  6902.,  5739., 10797.,   780.,  2675.],      dtype=float32)\n  batch_dim = 0"
     ]
    }
   ],
   "source": [
    "# train using stochastic gradient descent\n",
    "\n",
    "# set up\n",
    "N = len(targets_data)\n",
    "lr = 1.\n",
    "n_epochs = 5\n",
    "batch_size = 1000\n",
    "n_batches = N // batch_size\n",
    "\n",
    "epoch_losses = []\n",
    "\n",
    "# gradient descent\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # shuffle data\n",
    "    perm = np.random.permutation(N)\n",
    "    targets_epoch = targets_data[perm]\n",
    "    contexts_epoch = contexts_data[perm]\n",
    "    labels_epoch = labels_data[perm]\n",
    "\n",
    "    # decrease learning rate\n",
    "    if epoch % 2 == 0:\n",
    "        lr /= 2\n",
    "\n",
    "    # losses in each epoch\n",
    "    losses = []\n",
    "    for batch in tqdm(range(n_batches)):\n",
    "        targets_batch = targets_epoch[batch*batch_size: (batch+1)*batch_size]\n",
    "        contexts_batch = contexts_epoch[batch*batch_size: (batch+1)*batch_size]\n",
    "        labels_batch = labels_epoch[batch*batch_size: (batch+1)*batch_size]\n",
    "\n",
    "        # get batch embedding vectors\n",
    "        v_t_batch, u_c_batch = get_batch_embedding_vectors(targets_batch, contexts_batch, V, U)\n",
    "\n",
    "        # calculate batch_losses and batch_grads\n",
    "        loss_values = np.mean(batch_losses([v_t_batch, u_c_batch]))\n",
    "        grads = batch_grads([v_t_batch, u_c_batch])\n",
    "\n",
    "        # batch update parameters\n",
    "        partial_V_batch = grads[0]\n",
    "        partial_U_batch = grads[1]\n",
    "\n",
    "        batch_update_params(targets_batch, contexts_batch, partial_V_batch, partial_U_batch, V, U)\n",
    "\n",
    "        # store the loss value\n",
    "        losses.append(np.mean(loss_values))\n",
    "    \n",
    "    # store epoch losses\n",
    "    epoch_losses.append(np.mean(losses))\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # print epoch loss\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} \\t loss = {np.mean(epoch_losses)} \\t time = {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using gradient descent\n",
    "\n",
    "# set up\n",
    "N = len(targets_data)\n",
    "n_epochs = 5\n",
    "\n",
    "lr = 1.\n",
    "\n",
    "epoch_losses = []\n",
    "\n",
    "# gradient descent\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # shuffle data\n",
    "    perm = np.random.permutation(N)\n",
    "    targets = targets_data[perm]\n",
    "    contexts = contexts_data[perm]\n",
    "    labels = labels_data[perm]\n",
    "\n",
    "    losses = []\n",
    "    for i in tqdm(range(N)):\n",
    "        # stop after 1000 iterations\n",
    "        if i == 1000:\n",
    "            break\n",
    "        target = targets[i].astype(int) # ; print(f'target shape: {target.shape}')\n",
    "        context = contexts[i].astype(int)# ; print(f'context shape: {context}')\n",
    "        label = labels[i].astype(int)\n",
    "\n",
    "        # get the embedding vectors\n",
    "        v_t, u_c = get_embedding_vectors(target, context, V, U)\n",
    "\n",
    "        # get the loss value and gradient\n",
    "        loss_value = local_loss([v_t, u_c])\n",
    "        grad = L_grad([v_t, u_c])\n",
    "\n",
    "        # update the parameters\n",
    "        partial_V = grad[0]\n",
    "        partial_U = grad[1]\n",
    "        \n",
    "        V[:, target] = V[:, target] - lr * partial_V\n",
    "        U[context, :] = U[context, :] - lr * partial_U\n",
    "\n",
    "        # store the loss value\n",
    "        losses.append(loss_value)\n",
    "    \n",
    "    # store epoch losses\n",
    "    epoch_losses.append(np.mean(losses))\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # print epoch loss\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} \\t loss = {np.mean(epoch_losses)} \\t time = {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train using stochastic gradient descent\n",
    "\n",
    "# # number of training examples\n",
    "# N = len(targets_data)\n",
    "\n",
    "# # learning rate\n",
    "# lr = 3.\n",
    "\n",
    "# # number of epochs\n",
    "# n_epochs = 100\n",
    "\n",
    "# # batch size\n",
    "# batch_size = 10000\n",
    "\n",
    "# # number of batches per epoch\n",
    "# n_batches = N // batch_size\n",
    "\n",
    "# # keep track of losses\n",
    "# epoch_losses = []\n",
    "\n",
    "# # training the network\n",
    "# for epoch in range(n_epochs):\n",
    "#     start_time = time.time()\n",
    "#     # shuffle data\n",
    "#     perm = np.random.permutation(N)\n",
    "#     targets = targets[perm]\n",
    "#     contexts = contexts[perm]\n",
    "#     labels = labels[perm]\n",
    "\n",
    "#     # half the learning rate every 25 epochs\n",
    "#     if epoch == 50 or epoch == 75:\n",
    "#         lr /= 2.\n",
    "\n",
    "#     # losses in each epoch\n",
    "#     losses = []\n",
    "#     for batch in range(n_batches):\n",
    "#         targets_batch = targets_data[batch*batch_size: (batch+1)*batch_size]\n",
    "#         contexts_batch = contexts_data[batch*batch_size: (batch+1)*batch_size]\n",
    "#         labels_batch = labels_data[batch*batch_size: (batch+1)*batch_size]\n",
    "\n",
    "#         # calculate and save losses\n",
    "#         loss_value, gradient = loss_value_and_grad(params, targets_batch, contexts_batch)\n",
    "#         losses.append(loss_value)\n",
    "\n",
    "#         params = [(V - lr*dV, U - lr*dU) for (V, U), (dV, dU) in zip(params, gradient)]\n",
    "\n",
    "#     epoch_losses.append(np.mean(losses))\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     if epoch % 1 == 0:\n",
    "#         print(f\"Epoch {epoch+1}/{n_epochs} \\t loss = {np.mean(epoch_losses)} \\t time = {end_time - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V_ = np.copy(params[0][0])\n",
    "# U_ = np.copy(params[0][1])\n",
    "\n",
    "# # check dimensions of U and V, 10 epoch, 1r = 3, 1.5, 0.75, batch size = 500\n",
    "# print(f'V_ shape: {V_.shape}')\n",
    "# print(f'U_ shape: {U_.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy U and V\n",
    "V_trained = np.copy(params[0][0])\n",
    "U_trained = np.copy(params[0][1])\n",
    "\n",
    "# check dimensions of U and V, 100 epochs, lr = 1, batch_size = 500\n",
    "print(f'V_trained shape: {V_trained.shape}')\n",
    "print(f'U_trained shape: {U_trained.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate against test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function that takes in an index and vocab size and returns the one-hot encoding\n",
    "def getOneHot(index, vocab_size):\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[index] = 1\n",
    "    return onehot\n",
    "\n",
    "# define softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# test getOneHot and softmax function\n",
    "print(getOneHot(1, 10))\n",
    "print(softmax(np.array([1, 2, 3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions of U and V\n",
    "print(f'U shape: {U.shape}')\n",
    "print(f'V shape: {V.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see first 20 words in the vocab\n",
    "test_words = list(vocab.keys())[20:40]\n",
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cosine similarity scores between 2 word vectors\n",
    "def similarity_score(target_word_embedding, context_word_embedding):\n",
    "    return np.dot(target_word_embedding, context_word_embedding) / (np.linalg.norm(target_word_embedding) * np.linalg.norm(context_word_embedding))\n",
    "\n",
    "# define a function that find the most similar words to a given word\n",
    "def most_similar_words(word, V, n=5):\n",
    "    scores = []\n",
    "    target_word_idx = vocab[word]\n",
    "    for i in range(V.shape[1]):\n",
    "        if i == target_word_idx or inverse_vocab[i] == '<pad>':\n",
    "            continue\n",
    "        scores.append((inverse_vocab[i], similarity_score(V[:, target_word_idx], V[:, i])))\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check similarity between words\n",
    "# word: photos\n",
    "print(most_similar_words('amazed', V_trained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a forward pass through the skip-gram model\n",
    "\n",
    "# define the forward pass function\n",
    "def net(V, U, target_word_idx):\n",
    "    target_hot = getOneHot(target_word_idx, len(vocab))\n",
    "    return softmax( U @ V @ target_hot )\n",
    "\n",
    "def predict(word, V, U):\n",
    "    target_word_idx = vocab[word]\n",
    "    y_hat = net(V, U, target_word_idx)\n",
    "    # y_hat is the probability distribution over the vocab\n",
    "    # select the top 5 words with the highest probability\n",
    "    top_5 = np.argsort(y_hat)[-10:][::-1]\n",
    "    top_5_words = [inverse_vocab[i] for i in top_5]\n",
    "    return top_5_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 1 word from vocab\n",
    "word = np.random.choice(list(vocab.keys()))\n",
    "y_hat = net(V_trained, U_trained, vocab[word])\n",
    "print(f'Word: {word}')\n",
    "predict(word, V_trained, U_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
